{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e14bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and set up auto-reload\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e32e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_prompt\n",
    "from Autonomous_Learning_Agent.prompts import lead_researcher_prompt\n",
    "show_prompt(lead_researcher_prompt, \"Lead Researcher Prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9607e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/Autonomous_Learning_Agent/state_multi_agent_supervisor.py\n",
    "\n",
    "\"\"\"\n",
    "State Definitions for Multi-Agent Research Supervisor\n",
    "\n",
    "This module defines the state objects and tools used for the multi-agent\n",
    "research supervisor workflow, including coordination state and research tools.\n",
    "\"\"\"\n",
    "\n",
    "import operator\n",
    "from typing_extensions import Annotated, TypedDict, Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SupervisorState(TypedDict):\n",
    "    \"\"\"\n",
    "    State for the multi-agent research supervisor.\n",
    "    \n",
    "    Manages coordination between supervisor and research agents, tracking\n",
    "    research progress and accumulating findings from multiple sub-agents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Messages exchanged with supervisor for coordination and decision-making\n",
    "    supervisor_messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    # Detailed research brief that guides the overall research direction\n",
    "    research_brief: str\n",
    "    # Processed and structured notes ready for final report generation\n",
    "    notes: Annotated[list[str], operator.add] = []\n",
    "    # Counter tracking the number of research iterations performed\n",
    "    research_iterations: int = 0\n",
    "    # Raw unprocessed research notes collected from sub-agent research\n",
    "    raw_notes: Annotated[list[str], operator.add] = []\n",
    "\n",
    "@tool\n",
    "class ConductResearch(BaseModel):\n",
    "    \"\"\"Tool for delegating a research task to a specialized sub-agent.\"\"\"\n",
    "    research_topic: str = Field(\n",
    "        description=\"The topic to research. Should be a single topic, and should be described in high detail (at least a paragraph).\",\n",
    "    )\n",
    "\n",
    "@tool\n",
    "class ResearchComplete(BaseModel):\n",
    "    \"\"\"Tool for indicating that the research process is complete.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b4766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/Autonomous_Learning_Agent/multi_agent_supervisor.py\n",
    "\n",
    "\"\"\"Multi-agent supervisor for coordinating research across multiple specialized agents.\n",
    "\n",
    "This module implements a supervisor pattern where:\n",
    "1. A supervisor agent coordinates research activities and delegates tasks\n",
    "2. Multiple researcher agents work on specific sub-topics independently\n",
    "3. Results are aggregated and compressed for final reporting\n",
    "\n",
    "The supervisor uses parallel research execution to improve efficiency while\n",
    "maintaining isolated context windows for each research topic.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "\n",
    "from typing_extensions import Literal\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage, \n",
    "    BaseMessage, \n",
    "    SystemMessage, \n",
    "    ToolMessage,\n",
    "    filter_messages\n",
    ")\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "from Autonomous_Learning_Agent.prompts import lead_researcher_prompt\n",
    "from Autonomous_Learning_Agent.research_agent import researcher_agent\n",
    "from Autonomous_Learning_Agent.state_multi_agent_supervisor import (\n",
    "    SupervisorState, \n",
    "    ConductResearch, \n",
    "    ResearchComplete\n",
    ")\n",
    "from Autonomous_Learning_Agent.utils import get_today_str, think_tool\n",
    "\n",
    "def get_notes_from_tool_calls(messages: list[BaseMessage]) -> list[str]:\n",
    "    \"\"\"Extract research notes from ToolMessage objects in supervisor message history.\n",
    "    \n",
    "    This function retrieves the compressed research findings that sub-agents\n",
    "    return as ToolMessage content. When the supervisor delegates research to\n",
    "    sub-agents via ConductResearch tool calls, each sub-agent returns its\n",
    "    compressed findings as the content of a ToolMessage. This function\n",
    "    extracts all such ToolMessage content to compile the final research notes.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of messages from supervisor's conversation history\n",
    "        \n",
    "    Returns:\n",
    "        List of research note strings extracted from ToolMessage objects\n",
    "    \"\"\"\n",
    "    return [tool_msg.content for tool_msg in filter_messages(messages, include_types=\"tool\")]\n",
    "\n",
    "# Ensure async compatibility for Jupyter environments\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    # Only apply if running in Jupyter/IPython environment\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if get_ipython() is not None:\n",
    "            nest_asyncio.apply()\n",
    "    except ImportError:\n",
    "        pass  # Not in Jupyter, no need for nest_asyncio\n",
    "except ImportError:\n",
    "    pass  # nest_asyncio not available, proceed without it\n",
    "\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "\n",
    "supervisor_tools = [ConductResearch, ResearchComplete, think_tool]\n",
    "supervisor_model = init_chat_model(\"google_genai:models/gemini-flash-lite-latest\")\n",
    "supervisor_model_with_tools = supervisor_model.bind_tools(supervisor_tools)\n",
    "\n",
    "# System constants\n",
    "# Maximum number of tool call iterations for individual researcher agents\n",
    "# This prevents infinite loops and controls research depth per topic\n",
    "max_researcher_iterations = 6 # Calls to think_tool + ConductResearch\n",
    "\n",
    "# Maximum number of concurrent research agents the supervisor can launch\n",
    "# This is passed to the lead_researcher_prompt to limit parallel research tasks\n",
    "max_concurrent_researchers = 3\n",
    "\n",
    "# ===== SUPERVISOR NODES =====\n",
    "\n",
    "async def supervisor(state: SupervisorState) -> Command[Literal[\"supervisor_tools\"]]:\n",
    "    \"\"\"Coordinate research activities.\n",
    "    \n",
    "    Analyzes the research brief and current progress to decide:\n",
    "    - What research topics need investigation\n",
    "    - Whether to conduct parallel research\n",
    "    - When research is complete\n",
    "    \n",
    "    Args:\n",
    "        state: Current supervisor state with messages and research progress\n",
    "        \n",
    "    Returns:\n",
    "        Command to proceed to supervisor_tools node with updated state\n",
    "    \"\"\"\n",
    "    supervisor_messages = state.get(\"supervisor_messages\", [])\n",
    "    \n",
    "    # Prepare system message with current date and constraints\n",
    "    system_message = lead_researcher_prompt.format(\n",
    "        date=get_today_str(), \n",
    "        max_concurrent_research_units=max_concurrent_researchers,\n",
    "        max_researcher_iterations=max_researcher_iterations\n",
    "    )\n",
    "    messages = [SystemMessage(content=system_message)] + supervisor_messages\n",
    "    \n",
    "    # Make decision about next research steps\n",
    "    response = await supervisor_model_with_tools.ainvoke(messages)\n",
    "    \n",
    "    return Command(\n",
    "        goto=\"supervisor_tools\",\n",
    "        update={\n",
    "            \"supervisor_messages\": [response],\n",
    "            \"research_iterations\": state.get(\"research_iterations\", 0) + 1\n",
    "        }\n",
    "    )\n",
    "\n",
    "async def supervisor_tools(state: SupervisorState) -> Command[Literal[\"supervisor\", \"__end__\"]]:\n",
    "    \"\"\"Execute supervisor decisions - either conduct research or end the process.\n",
    "    \n",
    "    Handles:\n",
    "    - Executing think_tool calls for strategic reflection\n",
    "    - Launching parallel research agents for different topics\n",
    "    - Aggregating research results\n",
    "    - Determining when research is complete\n",
    "    \n",
    "    Args:\n",
    "        state: Current supervisor state with messages and iteration count\n",
    "        \n",
    "    Returns:\n",
    "        Command to continue supervision, end process, or handle errors\n",
    "    \"\"\"\n",
    "    supervisor_messages = state.get(\"supervisor_messages\", [])\n",
    "    research_iterations = state.get(\"research_iterations\", 0)\n",
    "    most_recent_message = supervisor_messages[-1]\n",
    "    \n",
    "    # Initialize variables for single return pattern\n",
    "    tool_messages = []\n",
    "    all_raw_notes = []\n",
    "    next_step = \"supervisor\"  # Default next step\n",
    "    should_end = False\n",
    "    \n",
    "    # Check exit criteria first\n",
    "    exceeded_iterations = research_iterations >= max_researcher_iterations\n",
    "    no_tool_calls = not most_recent_message.tool_calls\n",
    "    research_complete = any(\n",
    "        tool_call[\"name\"] == \"ResearchComplete\" \n",
    "        for tool_call in most_recent_message.tool_calls\n",
    "    )\n",
    "    \n",
    "    if exceeded_iterations or no_tool_calls or research_complete:\n",
    "        should_end = True\n",
    "        next_step = END\n",
    "    \n",
    "    else:\n",
    "        # Execute ALL tool calls before deciding next step\n",
    "        try:\n",
    "            # Separate think_tool calls from ConductResearch calls\n",
    "            think_tool_calls = [\n",
    "                tool_call for tool_call in most_recent_message.tool_calls \n",
    "                if tool_call[\"name\"] == \"think_tool\"\n",
    "            ]\n",
    "            \n",
    "            conduct_research_calls = [\n",
    "                tool_call for tool_call in most_recent_message.tool_calls \n",
    "                if tool_call[\"name\"] == \"ConductResearch\"\n",
    "            ]\n",
    "\n",
    "            # Handle think_tool calls (synchronous)\n",
    "            for tool_call in think_tool_calls:\n",
    "                observation = think_tool.invoke(tool_call[\"args\"])\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(\n",
    "                        content=observation,\n",
    "                        name=tool_call[\"name\"],\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Handle ConductResearch calls (asynchronous)\n",
    "            if conduct_research_calls:\n",
    "                # Launch parallel research agents\n",
    "                coros = [\n",
    "                    researcher_agent.ainvoke({\n",
    "                        \"researcher_messages\": [\n",
    "                            HumanMessage(content=tool_call[\"args\"][\"research_topic\"])\n",
    "                        ],\n",
    "                        \"research_topic\": tool_call[\"args\"][\"research_topic\"]\n",
    "                    }) \n",
    "                    for tool_call in conduct_research_calls\n",
    "                ]\n",
    "\n",
    "                # Wait for all research to complete\n",
    "                tool_results = await asyncio.gather(*coros)\n",
    "\n",
    "                # Format research results as tool messages\n",
    "                # Each sub-agent returns compressed research findings in result[\"compressed_research\"]\n",
    "                # We write this compressed research as the content of a ToolMessage, which allows\n",
    "                # the supervisor to later retrieve these findings via get_notes_from_tool_calls()\n",
    "                research_tool_messages = [\n",
    "                    ToolMessage(\n",
    "                        content=result.get(\"compressed_research\", \"Error synthesizing research report\"),\n",
    "                        name=tool_call[\"name\"],\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    ) for result, tool_call in zip(tool_results, conduct_research_calls)\n",
    "                ]\n",
    "                \n",
    "                tool_messages.extend(research_tool_messages)\n",
    "\n",
    "                # Aggregate raw notes from all research\n",
    "                all_raw_notes = [\n",
    "                    \"\\n\".join(result.get(\"raw_notes\", [])) \n",
    "                    for result in tool_results\n",
    "                ]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in supervisor tools: {e}\")\n",
    "            should_end = True\n",
    "            next_step = END\n",
    "    \n",
    "    # Single return point with appropriate state updates\n",
    "    if should_end:\n",
    "        return Command(\n",
    "            goto=next_step,\n",
    "            update={\n",
    "                \"notes\": get_notes_from_tool_calls(supervisor_messages),\n",
    "                \"research_brief\": state.get(\"research_brief\", \"\")\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=next_step,\n",
    "            update={\n",
    "                \"supervisor_messages\": tool_messages,\n",
    "                \"raw_notes\": all_raw_notes\n",
    "            }\n",
    "        )\n",
    "\n",
    "# ===== GRAPH CONSTRUCTION =====\n",
    "\n",
    "# Build supervisor graph\n",
    "supervisor_builder = StateGraph(SupervisorState)\n",
    "supervisor_builder.add_node(\"supervisor\", supervisor)\n",
    "supervisor_builder.add_node(\"supervisor_tools\", supervisor_tools)\n",
    "supervisor_builder.add_edge(START, \"supervisor\")\n",
    "supervisor_builder.add_edge(\"supervisor\", \"supervisor_tools\")\n",
    "supervisor_builder.add_edge(\"supervisor_tools\", END)\n",
    "supervisor_agent = supervisor_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from Autonomous_Learning_Agent.multi_agent_supervisor import supervisor_agent\n",
    "\n",
    "# Show the agent\n",
    "display(Image(supervisor_agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the multi-agent supervisor agent\n",
    "from utils import format_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "research_brief = \"\"\"I want to identify and evaluate the coffee shops in San Francisco that are considered the best based specifically  \n",
    "on coffee quality. My research should focus on analyzing and comparing coffee shops within the San Francisco area, \n",
    "using coffee quality as the primary criterion. I am open regarding methods of assessing coffee quality (e.g.,      \n",
    "expert reviews, customer ratings, specialty coffee certifications), and there are no constraints on ambiance,      \n",
    "location, wifi, or food options unless they directly impact perceived coffee quality. Please prioritize primary    \n",
    "sources such as the official websites of coffee shops, reputable third-party coffee review organizations (like     \n",
    "Coffee Review or Specialty Coffee Association), and prominent review aggregators like Google or Yelp where direct  \n",
    "customer feedback about coffee quality can be found. The study should result in a well-supported list or ranking of\n",
    "the top coffee shops in San Francisco, emphasizing their coffee quality according to the latest available data as  \n",
    "of July 2025.\"\"\"\n",
    "\n",
    "result = await supervisor_agent.ainvoke({\"supervisor_messages\": [HumanMessage(content=f\"{research_brief}.\")]})\n",
    "format_messages(result['supervisor_messages'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
