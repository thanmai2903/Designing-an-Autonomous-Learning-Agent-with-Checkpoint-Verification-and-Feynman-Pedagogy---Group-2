{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667795fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load environment variables and set up auto-reload\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7d34af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭───────────────────────────────────────────── </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Research Agent Prompt</span><span style=\"color: #000080; text-decoration-color: #000080\"> ─────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  You are a research assistant conducting research on the user's input topic. For context, today's date is       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  {date}.                                                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;Task&gt;</span>                                                                                                         <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  Your job is to use tools to gather information about the user's input topic.                                   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  You can use any of the tools provided to you to find resources that can help answer the research question.     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;/Task&gt;</span>                                                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;Available Tools&gt;</span>                                                                                              <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  You have access to two main tools:                                                                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  1. **tavily_search**: For conducting web searches to gather information                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  2. **think_tool**: For reflection and strategic planning during research                                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  **CRITICAL: Use think_tool after each search to reflect on results and plan next steps**                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;/Available Tools&gt;</span>                                                                                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;Instructions&gt;</span>                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  Think like a human researcher with limited time. Follow these steps:                                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  1. **Read the question carefully** - What specific information does the user need?                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  2. **Start with broader searches** - Use broad, comprehensive queries first                                    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  3. **After each search, pause and assess** - Do I have enough to answer? What's still missing?                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  4. **Execute narrower searches as you gather information** - Fill in the gaps                                  <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  5. **Stop when you can answer confidently** - Don't keep searching for perfection                              <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;/Instructions&gt;</span>                                                                                                <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;Hard Limits&gt;</span>                                                                                                  <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  **Tool Call Budgets** (Prevent excessive searching):                                                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **Simple queries**: Use 2-3 search tool calls maximum                                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **Complex queries**: Use up to 5 search tool calls maximum                                                   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **Always stop**: After 5 search tool calls if you cannot find the right sources                              <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  **Stop Immediately When**:                                                                                     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - You can answer the user's question comprehensively                                                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - You have 3+ relevant examples/sources for the question                                                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - Your last 2 searches returned similar information                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;/Hard Limits&gt;</span>                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;Show Your Thinking&gt;</span>                                                                                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  After each search tool call, use think_tool to analyze the results:                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - What key information did I find?                                                                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - What's missing?                                                                                              <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - Do I have enough to answer the question comprehensively?                                                     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - Should I search more or provide my answer?                                                                   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">&lt;/Show Your Thinking&gt;</span>                                                                                          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m────────────────────────────────────────────\u001b[0m\u001b[34m \u001b[0m\u001b[1;32mResearch Agent Prompt\u001b[0m\u001b[34m \u001b[0m\u001b[34m────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  You are a research assistant conducting research on the user's input topic. For context, today's date is       \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  {date}.                                                                                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m<Task>\u001b[0m                                                                                                         \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  Your job is to use tools to gather information about the user's input topic.                                   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  You can use any of the tools provided to you to find resources that can help answer the research question.     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m</Task>\u001b[0m                                                                                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m<Available Tools>\u001b[0m                                                                                              \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  You have access to two main tools:                                                                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  1. **tavily_search**: For conducting web searches to gather information                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  2. **think_tool**: For reflection and strategic planning during research                                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  **CRITICAL: Use think_tool after each search to reflect on results and plan next steps**                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m</Available Tools>\u001b[0m                                                                                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m<Instructions>\u001b[0m                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  Think like a human researcher with limited time. Follow these steps:                                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  1. **Read the question carefully** - What specific information does the user need?                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  2. **Start with broader searches** - Use broad, comprehensive queries first                                    \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  3. **After each search, pause and assess** - Do I have enough to answer? What's still missing?                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  4. **Execute narrower searches as you gather information** - Fill in the gaps                                  \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  5. **Stop when you can answer confidently** - Don't keep searching for perfection                              \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m</Instructions>\u001b[0m                                                                                                \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m<Hard Limits>\u001b[0m                                                                                                  \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  **Tool Call Budgets** (Prevent excessive searching):                                                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **Simple queries**: Use 2-3 search tool calls maximum                                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **Complex queries**: Use up to 5 search tool calls maximum                                                   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **Always stop**: After 5 search tool calls if you cannot find the right sources                              \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  **Stop Immediately When**:                                                                                     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - You can answer the user's question comprehensively                                                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - You have 3+ relevant examples/sources for the question                                                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - Your last 2 searches returned similar information                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m</Hard Limits>\u001b[0m                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m<Show Your Thinking>\u001b[0m                                                                                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  After each search tool call, use think_tool to analyze the results:                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - What key information did I find?                                                                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - What's missing?                                                                                              \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - Do I have enough to answer the question comprehensively?                                                     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - Should I search more or provide my answer?                                                                   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;34m</Show Your Thinking>\u001b[0m                                                                                          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import show_prompt\n",
    "from Autonomous_Learning_Agent.prompts import research_agent_prompt\n",
    "show_prompt(research_agent_prompt,\"Research Agent Prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7d1eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/Autonomous_Learning_Agent/state_research.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/Autonomous_Learning_Agent/state_research.py\n",
    "\n",
    "\"\"\"\n",
    "State Definitions and Pydantic Schemas for Research Agent\n",
    "\n",
    "This module defines the state objects and structured schemas used for\n",
    "the research agent workflow, including researcher state management and output schemas.\n",
    "\"\"\"\n",
    "\n",
    "import operator\n",
    "from typing_extensions import TypedDict, Annotated, List, Sequence\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# ===== STATE DEFINITIONS =====\n",
    "\n",
    "class ResearcherState(TypedDict):\n",
    "    \"\"\"\n",
    "    State for the research agent containing message history and research metadata.\n",
    "    \n",
    "    This state tracks the researcher's conversation, iteration count for limiting\n",
    "    tool calls, the research topic being investigated, compressed findings,\n",
    "    and raw research notes for detailed analysis.\n",
    "    \"\"\"\n",
    "    researcher_messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    tool_call_iterations: int\n",
    "    research_topic: str\n",
    "    compressed_research: str\n",
    "    raw_notes: Annotated[List[str], operator.add]\n",
    "\n",
    "class ResearcherOutputState(TypedDict):\n",
    "    \"\"\"\n",
    "    Output state for the research agent containing final research results.\n",
    "    \n",
    "    This represents the final output of the research process with compressed\n",
    "    research findings and all raw notes from the research process.\n",
    "    \"\"\"\n",
    "    compressed_research: str\n",
    "    raw_notes: Annotated[List[str], operator.add]\n",
    "    researcher_messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# ===== STRUCTURED OUTPUT SCHEMAS =====\n",
    "\n",
    "class ClarifyWithUser(BaseModel):\n",
    "    \"\"\"Schema for user clarification decisions during scoping phase.\"\"\"\n",
    "    need_clarification: bool = Field(\n",
    "        description=\"Whether the user needs to be asked a clarifying question.\",\n",
    "    )\n",
    "    question: str = Field(\n",
    "        description=\"A question to ask the user to clarify the report scope\",\n",
    "    )\n",
    "    verification: str = Field(\n",
    "        description=\"Verify message that we will start research after the user has provided the necessary information.\",\n",
    "    )\n",
    "\n",
    "class ResearchQuestion(BaseModel):\n",
    "    \"\"\"Schema for research brief generation.\"\"\"\n",
    "    research_brief: str = Field(\n",
    "        description=\"A research question that will be used to guide the research.\",\n",
    "    )\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    \"\"\"Schema for webpage content summarization.\"\"\"\n",
    "    summary: str = Field(description=\"Concise summary of the webpage content\")\n",
    "    key_excerpts: str = Field(description=\"Important quotes and excerpts from the content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "092f8a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/Autonomous_Learning_Agent/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/Autonomous_Learning_Agent/utils.py\n",
    "\n",
    "\"\"\"Research Utilities and Tools.\n",
    "\n",
    "This module provides search and content processing utilities for the research agent,\n",
    "including web search capabilities and content summarization tools.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing_extensions import Annotated, List, Literal\n",
    "\n",
    "from langchain.chat_models import init_chat_model \n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool, InjectedToolArg\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from Autonomous_Learning_Agent.state_research import Summary\n",
    "from Autonomous_Learning_Agent.prompts import summarize_webpage_prompt\n",
    "\n",
    "# ===== UTILITY FUNCTIONS =====\n",
    "\n",
    "def get_today_str() -> str:\n",
    "    \"\"\"Get current date in a human-readable format.\"\"\"\n",
    "    return datetime.now().strftime(\"%a %b %#d, %Y\")\n",
    "\n",
    "def get_current_dir() -> Path:\n",
    "    \"\"\"Get the current directory of the module.\n",
    "\n",
    "    This function is compatible with Jupyter notebooks and regular Python scripts.\n",
    "\n",
    "    Returns:\n",
    "        Path object representing the current directory\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return Path(__file__).resolve().parent\n",
    "    except NameError:  # __file__ is not defined\n",
    "        return Path.cwd()\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "\n",
    "summarization_model = init_chat_model(\"google_genai:models/gemini-flash-lite-latest\")\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "# ===== SEARCH FUNCTIONS =====\n",
    "\n",
    "def tavily_search_multiple(\n",
    "    search_queries: List[str], \n",
    "    max_results: int = 3, \n",
    "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\", \n",
    "    include_raw_content: bool = True, \n",
    ") -> List[dict]:\n",
    "    \"\"\"Perform search using Tavily API for multiple queries.\n",
    "\n",
    "    Args:\n",
    "        search_queries: List of search queries to execute\n",
    "        max_results: Maximum number of results per query\n",
    "        topic: Topic filter for search results\n",
    "        include_raw_content: Whether to include raw webpage content\n",
    "\n",
    "    Returns:\n",
    "        List of search result dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute searches sequentially. Note: yon can use AsyncTavilyClient to parallelize this step.\n",
    "    search_docs = []\n",
    "    for query in search_queries:\n",
    "        result = tavily_client.search(\n",
    "            query,\n",
    "            max_results=max_results,\n",
    "            include_raw_content=include_raw_content,\n",
    "            topic=topic\n",
    "        )\n",
    "        search_docs.append(result)\n",
    "\n",
    "    return search_docs\n",
    "\n",
    "def summarize_webpage_content(webpage_content: str) -> str:\n",
    "    \"\"\"Summarize webpage content using the configured summarization model.\n",
    "    \n",
    "    Args:\n",
    "        webpage_content: Raw webpage content to summarize\n",
    "        \n",
    "    Returns:\n",
    "        Formatted summary with key excerpts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up structured output model for summarization\n",
    "        structured_model = summarization_model.with_structured_output(Summary)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = structured_model.invoke([\n",
    "            HumanMessage(content=summarize_webpage_prompt.format(\n",
    "                webpage_content=webpage_content, \n",
    "                date=get_today_str()\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # Format summary with clear structure\n",
    "        formatted_summary = (\n",
    "            f\"<summary>\\n{summary.summary}\\n</summary>\\n\\n\"\n",
    "            f\"<key_excerpts>\\n{summary.key_excerpts}\\n</key_excerpts>\"\n",
    "        )\n",
    "        \n",
    "        return formatted_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to summarize webpage: {str(e)}\")\n",
    "        return webpage_content[:1000] + \"...\" if len(webpage_content) > 1000 else webpage_content\n",
    "\n",
    "def deduplicate_search_results(search_results: List[dict]) -> dict:\n",
    "    \"\"\"Deduplicate search results by URL to avoid processing duplicate content.\n",
    "    \n",
    "    Args:\n",
    "        search_results: List of search result dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping URLs to unique results\n",
    "    \"\"\"\n",
    "    unique_results = {}\n",
    "    \n",
    "    for response in search_results:\n",
    "        for result in response['results']:\n",
    "            url = result['url']\n",
    "            if url not in unique_results:\n",
    "                unique_results[url] = result\n",
    "    \n",
    "    return unique_results\n",
    "\n",
    "def process_search_results(unique_results: dict) -> dict:\n",
    "    \"\"\"Process search results by summarizing content where available.\n",
    "    \n",
    "    Args:\n",
    "        unique_results: Dictionary of unique search results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of processed results with summaries\n",
    "    \"\"\"\n",
    "    summarized_results = {}\n",
    "    \n",
    "    for url, result in unique_results.items():\n",
    "        # Use existing content if no raw content for summarization\n",
    "        if not result.get(\"raw_content\"):\n",
    "            content = result['content']\n",
    "        else:\n",
    "            # Summarize raw content for better processing\n",
    "            content = summarize_webpage_content(result['raw_content'])\n",
    "        \n",
    "        summarized_results[url] = {\n",
    "            'title': result['title'],\n",
    "            'content': content\n",
    "        }\n",
    "    \n",
    "    return summarized_results\n",
    "\n",
    "def format_search_output(summarized_results: dict) -> str:\n",
    "    \"\"\"Format search results into a well-structured string output.\n",
    "    \n",
    "    Args:\n",
    "        summarized_results: Dictionary of processed search results\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string of search results with clear source separation\n",
    "    \"\"\"\n",
    "    if not summarized_results:\n",
    "        return \"No valid search results found. Please try different search queries or use a different search API.\"\n",
    "    \n",
    "    formatted_output = \"Search results: \\n\\n\"\n",
    "    \n",
    "    for i, (url, result) in enumerate(summarized_results.items(), 1):\n",
    "        formatted_output += f\"\\n\\n--- SOURCE {i}: {result['title']} ---\\n\"\n",
    "        formatted_output += f\"URL: {url}\\n\\n\"\n",
    "        formatted_output += f\"SUMMARY:\\n{result['content']}\\n\\n\"\n",
    "        formatted_output += \"-\" * 80 + \"\\n\"\n",
    "    \n",
    "    return formatted_output\n",
    "\n",
    "# ===== RESEARCH TOOLS =====\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def tavily_search(\n",
    "    query: str,\n",
    "    max_results: Annotated[int, InjectedToolArg] = 3,\n",
    "    topic: Annotated[Literal[\"general\", \"news\", \"finance\"], InjectedToolArg] = \"general\",\n",
    ") -> str:\n",
    "    \"\"\"Fetch results from Tavily search API with content summarization.\n",
    "\n",
    "    Args:\n",
    "        query: A single search query to execute\n",
    "        max_results: Maximum number of results to return\n",
    "        topic: Topic to filter results by ('general', 'news', 'finance')\n",
    "\n",
    "    Returns:\n",
    "        Formatted string of search results with summaries\n",
    "    \"\"\"\n",
    "    # Execute search for single query\n",
    "    search_results = tavily_search_multiple(\n",
    "        [query],  # Convert single query to list for the internal function\n",
    "        max_results=max_results,\n",
    "        topic=topic,\n",
    "        include_raw_content=True,\n",
    "    )\n",
    "\n",
    "    # Deduplicate results by URL to avoid processing duplicate content\n",
    "    unique_results = deduplicate_search_results(search_results)\n",
    "\n",
    "    # Process results with summarization\n",
    "    summarized_results = process_search_results(unique_results)\n",
    "\n",
    "    # Format output for consumption\n",
    "    return format_search_output(summarized_results)\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def think_tool(reflection: str) -> str:\n",
    "    \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
    "    \n",
    "    Use this tool after each search to analyze results and plan next steps systematically.\n",
    "    This creates a deliberate pause in the research workflow for quality decision-making.\n",
    "    \n",
    "    When to use:\n",
    "    - After receiving search results: What key information did I find?\n",
    "    - Before deciding next steps: Do I have enough to answer comprehensively?\n",
    "    - When assessing research gaps: What specific information am I still missing?\n",
    "    - Before concluding research: Can I provide a complete answer now?\n",
    "    \n",
    "    Reflection should address:\n",
    "    1. Analysis of current findings - What concrete information have I gathered?\n",
    "    2. Gap assessment - What crucial information is still missing?\n",
    "    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
    "    4. Strategic decision - Should I continue searching or provide my answer?\n",
    "    \n",
    "    Args:\n",
    "        reflection: Your detailed reflection on research progress, findings, gaps, and next steps\n",
    "        \n",
    "    Returns:\n",
    "        Confirmation that reflection was recorded for decision-making\n",
    "    \"\"\"\n",
    "    return f\"Reflection recorded: {reflection}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ccbaf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/Autonomous_Learning_Agent/research_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/Autonomous_Learning_Agent/research_agent.py\n",
    "\n",
    "\"\"\"Research Agent Implementation.\n",
    "\n",
    "This module implements a research agent that can perform iterative web searches\n",
    "and synthesis to answer complex research questions.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Literal\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, filter_messages\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from Autonomous_Learning_Agent.state_research import ResearcherState, ResearcherOutputState\n",
    "from Autonomous_Learning_Agent.utils import tavily_search, get_today_str, think_tool\n",
    "from Autonomous_Learning_Agent.prompts import research_agent_prompt, compress_research_system_prompt, compress_research_human_message\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "\n",
    "# Set up tools and model binding\n",
    "tools = [tavily_search, think_tool]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "# Initialize models\n",
    "model = init_chat_model(\"google_genai:models/gemini-flash-lite-latest\")\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "summarization_model = init_chat_model(\"google_genai:models/gemini-flash-lite-latest\")\n",
    "compress_model = init_chat_model(\"google_genai:models/gemini-flash-lite-latest\") # model=\"anthropic:claude-sonnet-4-20250514\", max_tokens=64000\n",
    "\n",
    "# ===== AGENT NODES =====\n",
    "\n",
    "def llm_call(state: ResearcherState):\n",
    "    \"\"\"Analyze current state and decide on next actions.\n",
    "    \n",
    "    The model analyzes the current conversation state and decides whether to:\n",
    "    1. Call search tools to gather more information\n",
    "    2. Provide a final answer based on gathered information\n",
    "    \n",
    "    Returns updated state with the model's response.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"researcher_messages\": [\n",
    "            model_with_tools.invoke(\n",
    "                [SystemMessage(content=research_agent_prompt)] + state[\"researcher_messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def tool_node(state: ResearcherState):\n",
    "    \"\"\"Execute all tool calls from the previous LLM response.\n",
    "    \n",
    "    Executes all tool calls from the previous LLM responses.\n",
    "    Returns updated state with tool execution results.\n",
    "    \"\"\"\n",
    "    tool_calls = state[\"researcher_messages\"][-1].tool_calls\n",
    " \n",
    "    # Execute all tool calls\n",
    "    observations = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observations.append(tool.invoke(tool_call[\"args\"]))\n",
    "            \n",
    "    # Create tool message outputs\n",
    "    tool_outputs = [\n",
    "        ToolMessage(\n",
    "            content=observation,\n",
    "            name=tool_call[\"name\"],\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        ) for observation, tool_call in zip(observations, tool_calls)\n",
    "    ]\n",
    "    \n",
    "    return {\"researcher_messages\": tool_outputs}\n",
    "\n",
    "def compress_research(state: ResearcherState) -> dict:\n",
    "    \"\"\"Compress research findings into a concise summary.\n",
    "    \n",
    "    Takes all the research messages and tool outputs and creates\n",
    "    a compressed summary suitable for the supervisor's decision-making.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_message = compress_research_system_prompt.format(date=get_today_str())\n",
    "    messages = [SystemMessage(content=system_message)] + state.get(\"researcher_messages\", []) + [HumanMessage(content=compress_research_human_message)]\n",
    "    response = compress_model.invoke(messages)\n",
    "    \n",
    "    # Extract raw notes from tool and AI messages\n",
    "    raw_notes = [\n",
    "        str(m.content) for m in filter_messages(\n",
    "            state[\"researcher_messages\"], \n",
    "            include_types=[\"tool\", \"ai\"]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"compressed_research\": str(response.content),\n",
    "        \"raw_notes\": [\"\\n\".join(raw_notes)]\n",
    "    }\n",
    "\n",
    "# ===== ROUTING LOGIC =====\n",
    "\n",
    "def should_continue(state: ResearcherState) -> Literal[\"tool_node\", \"compress_research\"]:\n",
    "    \"\"\"Determine whether to continue research or provide final answer.\n",
    "    \n",
    "    Determines whether the agent should continue the research loop or provide\n",
    "    a final answer based on whether the LLM made tool calls.\n",
    "    \n",
    "    Returns:\n",
    "        \"tool_node\": Continue to tool execution\n",
    "        \"compress_research\": Stop and compress research\n",
    "    \"\"\"\n",
    "    messages = state[\"researcher_messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the LLM makes a tool call, continue to tool execution\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "    # Otherwise, we have a final answer\n",
    "    return \"compress_research\"\n",
    "\n",
    "# ===== GRAPH CONSTRUCTION =====\n",
    "\n",
    "# Build the agent workflow\n",
    "agent_builder = StateGraph(ResearcherState, output_schema=ResearcherOutputState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "agent_builder.add_node(\"compress_research\", compress_research)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tool_node\": \"tool_node\", # Continue research loop\n",
    "        \"compress_research\": \"compress_research\", # Provide final answer\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_call\") # Loop back for more research\n",
    "agent_builder.add_edge(\"compress_research\", END)\n",
    "\n",
    "# Compile the agent\n",
    "researcher_agent = agent_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a613bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from Autonomous_Learning_Agent.research_agent import researcher_agent\n",
    "\n",
    "# Show the agent\n",
    "display(Image(researcher_agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb44f45a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Error calling model 'models/gemini-flash-lite-latest' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 10.00864279s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '10s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:3040\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3039\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3040\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3042\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\google\\genai\\models.py:5188\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5181\u001b[39m     logger.warning(\n\u001b[32m   5182\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mTools at indices [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m] are not compatible with automatic function \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   5183\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcalling (AFC). AFC is disabled. If AFC is intended, please \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5186\u001b[39m         indices_str,\n\u001b[32m   5187\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m5188\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5189\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5190\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5192\u001b[39m remaining_remote_calls_afc = _extra_utils.get_max_remote_calls_afc(\n\u001b[32m   5193\u001b[39m     parsed_config\n\u001b[32m   5194\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\google\\genai\\models.py:3985\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3983\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m3985\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3986\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   3990\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3991\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1388\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1385\u001b[39m http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1386\u001b[39m     http_method, path, request_dict, http_options\n\u001b[32m   1387\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m response_body = (\n\u001b[32m   1390\u001b[39m     response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1391\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1222\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1221\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\BoldServices\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\BoldServices\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\google\\genai\\_api_client.py:1201\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1194\u001b[39m response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1195\u001b[39m     method=http_request.method,\n\u001b[32m   1196\u001b[39m     url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1199\u001b[39m     timeout=http_request.timeout,\n\u001b[32m   1200\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1203\u001b[39m     response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1204\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\google\\genai\\errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\google\\genai\\errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 10.00864279s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '10s'}]}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Example brief\u001b[39;00m\n\u001b[32m      6\u001b[39m research_brief = \u001b[33m\"\"\"\u001b[39m\u001b[33mI want to identify and evaluate the coffee shops in San Francisco that are considered the best based specifically  \u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mon coffee quality. My research should focus on analyzing and comparing coffee shops within the San Francisco area, \u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33musing coffee quality as the primary criterion. I am open regarding methods of assessing coffee quality (e.g.,      \u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[33mthe top coffee shops in San Francisco, emphasizing their coffee quality according to the latest available data as  \u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33mof July 2025.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result = \u001b[43mresearcher_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresearcher_messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mresearch_brief\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m format_messages(result[\u001b[33m'\u001b[39m\u001b[33mresearcher_messages\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\src\\Autonomous_Learning_Agent\\research_agent.py:44\u001b[39m, in \u001b[36mllm_call\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm_call\u001b[39m(state: ResearcherState):\n\u001b[32m     34\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Analyze current state and decide on next actions.\u001b[39;00m\n\u001b[32m     35\u001b[39m \n\u001b[32m     36\u001b[39m \u001b[33;03m    The model analyzes the current conversation state and decides whether to:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m \u001b[33;03m    Returns updated state with the model's response.\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     43\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresearcher_messages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m             \u001b[43mmodel_with_tools\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresearch_agent_prompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresearcher_messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m         ]\n\u001b[32m     48\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5557\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5550\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5552\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5555\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5556\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5558\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5559\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5560\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2529\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   2526\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2527\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m2529\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:3044\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3040\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28mself\u001b[39m.client.models.generate_content(\n\u001b[32m   3041\u001b[39m         **request,\n\u001b[32m   3042\u001b[39m     )\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m3044\u001b[39m     \u001b[43m_handle_client_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Aspire\\Internships\\Projects\\Learning Agent- Group 1\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:145\u001b[39m, in \u001b[36m_handle_client_error\u001b[39m\u001b[34m(e, request)\u001b[39m\n\u001b[32m    143\u001b[39m model_name = request.get(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError calling model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Error calling model 'models/gemini-flash-lite-latest' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash-lite\\nPlease retry in 10.00864279s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash-lite'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '10s'}]}}",
      "During task with name 'llm_call' and id '319a118f-ab85-1be4-8236-2253fc7ee247'"
     ]
    }
   ],
   "source": [
    "# Run the agent\n",
    "from utils import format_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Example brief\n",
    "research_brief = \"\"\"I want to identify and evaluate the coffee shops in San Francisco that are considered the best based specifically  \n",
    "on coffee quality. My research should focus on analyzing and comparing coffee shops within the San Francisco area, \n",
    "using coffee quality as the primary criterion. I am open regarding methods of assessing coffee quality (e.g.,      \n",
    "expert reviews, customer ratings, specialty coffee certifications), and there are no constraints on ambiance,      \n",
    "location, wifi, or food options unless they directly impact perceived coffee quality. Please prioritize primary    \n",
    "sources such as the official websites of coffee shops, reputable third-party coffee review organizations (like     \n",
    "Coffee Review or Specialty Coffee Association), and prominent review aggregators like Google or Yelp where direct  \n",
    "customer feedback about coffee quality can be found. The study should result in a well-supported list or ranking of\n",
    "the top coffee shops in San Francisco, emphasizing their coffee quality according to the latest available data as  \n",
    "of July 2025.\"\"\"\n",
    "\n",
    "result = researcher_agent.invoke({\"researcher_messages\": [HumanMessage(content=f\"{research_brief}.\")]})\n",
    "format_messages(result['researcher_messages'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
