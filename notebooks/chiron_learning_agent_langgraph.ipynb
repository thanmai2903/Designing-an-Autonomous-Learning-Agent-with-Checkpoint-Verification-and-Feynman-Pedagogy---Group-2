{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ca3aa8-3150-4f2c-88fe-c8d1211ab462",
   "metadata": {},
   "source": [
    "# Chiron - A Feynman-Enhanced Learning Agent Using LangGraph\n",
    "\n",
    "## Overview\n",
    "This notebook presents a structured learning agent implemented using LangGraph. The system guides learners through a sequence of defined but customizable checkpoints, verifying understanding at each step and providing Feynman-style teaching when needed.\n",
    "\n",
    "## Motivation\n",
    "In traditional educational settings, access to personalized 1:1 tutoring is often limited by cost and availability. This project aims to democratize personalized learning by creating an AI tutor that can:\n",
    "- Provide individualized attention and feedback 24/7\n",
    "- Use your own notes and web-retrieved content as context\n",
    "- Offer patient, simple explanations of complex topics\n",
    "\n",
    "## Key Components\n",
    "1. **Learning State Graph**: Orchestrates the sequential learning workflow\n",
    "2. **Checkpoint System**: Defines structured learning milestones\n",
    "3. **Web Search Integration**: Dynamically retrieves relevant learning materials\n",
    "4. **Context Processing**: Chunks and processes learning materials\n",
    "5. **Question Generation**: Creates checkpoint-specific verification questions\n",
    "6. **Understanding Verification**: Evaluates learner comprehension with a clear threshold (70%)\n",
    "7. **Feynman Teaching**: Simplifies complex concepts when understanding is insufficient\n",
    "\n",
    "## Method\n",
    "The system follows a structured learning cycle:\n",
    "\n",
    "### 1. Checkpoint Definition\n",
    "- Generates sequential learning milestones with clear success criteria\n",
    "\n",
    "### 2. Context Building\n",
    "- Processes student-provided materials or retrieves relevant web content\n",
    "\n",
    "### 3. Context Validation\n",
    "- Validates context based on checkpoint criteria\n",
    "- Performs additional web searches if context doesn't meet checkpoint requirements\n",
    "\n",
    "### 4. Embedding Storage\n",
    "- Stores embeddings for retrieving only relevant chunks during verification\n",
    "\n",
    "### 5. Understanding Verification\n",
    "- Generates checkpoint-specific questions\n",
    "- Evaluates responses against a 70% understanding threshold\n",
    "- Provides detailed feedback\n",
    "\n",
    "### 6. Progressive Learning\n",
    "- Advances to the next checkpoint when understanding is verified\n",
    "- Provides Feynman-style explanations when needed\n",
    "\n",
    "## Conclusion\n",
    "This notebook demonstrates a structured approach to guided learning. By combining sequential checkpoints, clear verification thresholds, and Feynman-style teaching, it offers a methodical learning experience with immediate support when needed. The system is particularly effective for complex topics that benefit from step-by-step guidance and simplified explanations.\n",
    "\n",
    "![Chiron](../images/chiron.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd42e1e-fd75-4719-8590-fd912560482f",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcfe0224-298f-480f-93b4-b74a1058f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-community langchain-openai langgraph pydantic python-dotenv semantic-chunkers semantic-router tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c377c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac466075-3839-4c1b-b135-12edd7c8b3b7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca96cc5-2062-4223-b64f-d3d0c2a33039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "import uuid\n",
    "from typing import Annotated, Dict, List, Optional, Tuple, TypedDict\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "from semantic_chunkers import StatisticalChunker\n",
    "# from semantic_router.encoders import OpenAIEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72c4fbc9-508f-43f4-9a8d-a5fcae4c50c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semantic-router[google] in d:\\aspire\\conda_dire\\lib\\site-packages (0.1.12)\n",
      "Requirement already satisfied: aiohttp<4,>=3.10.11 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (3.13.2)\n",
      "Requirement already satisfied: aurelio-sdk>=0.0.19 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (0.0.19)\n",
      "Requirement already satisfied: colorama<0.5,>=0.4.6 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (0.4.6)\n",
      "Requirement already satisfied: colorlog<7,>=6.8.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (6.10.1)\n",
      "Requirement already satisfied: litellm>=1.61.3 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (1.80.11)\n",
      "Requirement already satisfied: numpy>=1.25.2 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (2.3.5)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.10.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (2.14.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.10.2 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (2.12.4)\n",
      "Requirement already satisfied: pyyaml<7,>=6.0.1 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (6.0.3)\n",
      "Requirement already satisfied: regex>=2023.12.25 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (2025.9.1)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.6.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (0.12.0)\n",
      "Requirement already satisfied: tornado<7,>=6.4.2 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (6.5.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router[google]) (2.5.0)\n",
      "Collecting google-cloud-aiplatform<2,>=1.45.0 (from semantic-router[google])\n",
      "  Using cached google_cloud_aiplatform-1.132.0-py2.py3-none-any.whl.metadata (46 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from aiohttp<4,>=3.10.11->semantic-router[google]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from aiohttp<4,>=3.10.11->semantic-router[google]) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from aiohttp<4,>=3.10.11->semantic-router[google]) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\aspire\\conda_dire\\lib\\site-packages (from aiohttp<4,>=3.10.11->semantic-router[google]) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\aspire\\conda_dire\\lib\\site-packages (from aiohttp<4,>=3.10.11->semantic-router[google]) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from aiohttp<4,>=3.10.11->semantic-router[google]) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from aiohttp<4,>=3.10.11->semantic-router[google]) (1.22.0)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Using cached google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.45.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (2.47.0)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Downloading proto_plus-1.27.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (5.29.3)\n",
      "Requirement already satisfied: packaging>=14.3 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (25.0)\n",
      "Collecting google-cloud-storage<4.0.0,>=2.10.0 (from google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Downloading google_cloud_storage-3.7.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 (from google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Downloading google_cloud_bigquery-3.39.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0,>=1.3.3 (from google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Downloading google_cloud_resource_manager-1.15.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting shapely<3.0.0 (from google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Using cached shapely-2.1.2-cp313-cp313-win_amd64.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (1.56.0)\n",
      "Requirement already satisfied: typing_extensions in d:\\aspire\\conda_dire\\lib\\site-packages (from google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (4.15.0)\n",
      "Collecting docstring_parser<1 (from google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Using cached googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (1.67.1)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Using cached grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (4.9.1)\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.1 (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Downloading google_cloud_core-2.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.0.0 (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Downloading google_resumable_media-2.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (2.9.0.post0)\n",
      "Collecting grpc-google-iam-v1<1.0.0,>=0.14.0 (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Downloading grpc_google_iam_v1-0.14.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting google-crc32c<2.0.0,>=1.1.3 (from google-cloud-storage<4.0.0,>=2.10.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Downloading google_crc32c-1.8.0-cp313-cp313-win_amd64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (4.10.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (0.28.1)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (15.0.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (1.9.0)\n",
      "Requirement already satisfied: sniffio in d:\\aspire\\conda_dire\\lib\\site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\aspire\\conda_dire\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (3.11)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Using cached protobuf-6.33.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google])\n",
      "  Using cached grpcio-1.76.0-cp313-cp313-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: certifi in d:\\aspire\\conda_dire\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\aspire\\conda_dire\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\aspire\\conda_dire\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (0.16.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from openai<3.0.0,>=1.10.0->semantic-router[google]) (0.12.0)\n",
      "Requirement already satisfied: tqdm>4 in d:\\aspire\\conda_dire\\lib\\site-packages (from openai<3.0.0,>=1.10.0->semantic-router[google]) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from pydantic<3,>=2.10.2->semantic-router[google]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\aspire\\conda_dire\\lib\\site-packages (from pydantic<3,>=2.10.2->semantic-router[google]) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\aspire\\conda_dire\\lib\\site-packages (from pydantic<3,>=2.10.2->semantic-router[google]) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\aspire\\conda_dire\\lib\\site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\aspire\\conda_dire\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (3.4.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\aspire\\conda_dire\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform<2,>=1.45.0->semantic-router[google]) (0.6.1)\n",
      "Requirement already satisfied: aiofiles<25.0.0,>=24.1.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from aurelio-sdk>=0.0.19->semantic-router[google]) (24.1.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in d:\\aspire\\conda_dire\\lib\\site-packages (from aurelio-sdk>=0.0.19->semantic-router[google]) (1.1.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from aurelio-sdk>=0.0.19->semantic-router[google]) (1.0.0)\n",
      "Requirement already satisfied: click in d:\\aspire\\conda_dire\\lib\\site-packages (from litellm>=1.61.3->semantic-router[google]) (8.2.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from litellm>=1.61.3->semantic-router[google]) (0.14.0)\n",
      "INFO: pip is looking at multiple versions of litellm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting litellm>=1.61.3 (from semantic-router[google])\n",
      "  Downloading litellm-1.80.10-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading litellm-1.80.9-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading litellm-1.80.8-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading litellm-1.80.7-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading litellm-1.80.6-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading litellm-1.80.5-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from litellm>=1.61.3->semantic-router[google]) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in d:\\aspire\\conda_dire\\lib\\site-packages (from litellm>=1.61.3->semantic-router[google]) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from litellm>=1.61.3->semantic-router[google]) (4.25.0)\n",
      "Requirement already satisfied: tokenizers in d:\\aspire\\conda_dire\\lib\\site-packages (from litellm>=1.61.3->semantic-router[google]) (0.22.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.61.3->semantic-router[google]) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\aspire\\conda_dire\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.61.3->semantic-router[google]) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\aspire\\conda_dire\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.61.3->semantic-router[google]) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\aspire\\conda_dire\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.61.3->semantic-router[google]) (0.28.0)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\aspire\\conda_dire\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm>=1.61.3->semantic-router[google]) (3.23.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in d:\\aspire\\conda_dire\\lib\\site-packages (from tokenizers->litellm>=1.61.3->semantic-router[google]) (1.2.4)\n",
      "Requirement already satisfied: filelock in d:\\aspire\\conda_dire\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.61.3->semantic-router[google]) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.61.3->semantic-router[google]) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.61.3->semantic-router[google]) (1.2.0)\n",
      "Requirement already satisfied: shellingham in d:\\aspire\\conda_dire\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.61.3->semantic-router[google]) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in d:\\aspire\\conda_dire\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.61.3->semantic-router[google]) (0.21.1)\n",
      "Downloading google_cloud_aiplatform-1.132.0-py2.py3-none-any.whl (8.2 MB)\n",
      "   ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 4.5/8.2 MB 21.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.2 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.2/8.2 MB 15.7 MB/s  0:00:00\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Using cached google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "Downloading google_cloud_bigquery-3.39.0-py3-none-any.whl (259 kB)\n",
      "Downloading google_cloud_core-2.5.0-py3-none-any.whl (29 kB)\n",
      "Downloading google_cloud_resource_manager-1.15.0-py3-none-any.whl (397 kB)\n",
      "Downloading google_cloud_storage-3.7.0-py3-none-any.whl (303 kB)\n",
      "Downloading google_crc32c-1.8.0-cp313-cp313-win_amd64.whl (34 kB)\n",
      "Downloading google_resumable_media-2.8.0-py3-none-any.whl (81 kB)\n",
      "Using cached googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Downloading grpc_google_iam_v1-0.14.3-py3-none-any.whl (32 kB)\n",
      "Using cached grpcio_status-1.76.0-py3-none-any.whl (14 kB)\n",
      "Using cached grpcio-1.76.0-cp313-cp313-win_amd64.whl (4.7 MB)\n",
      "Downloading proto_plus-1.27.0-py3-none-any.whl (50 kB)\n",
      "Using cached protobuf-6.33.2-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Using cached shapely-2.1.2-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "Downloading litellm-1.80.5-py3-none-any.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 3.7/10.7 MB 18.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/10.7 MB 16.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.8/10.7 MB 819.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 8.1/10.7 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 10.2/10.7 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 1.4 MB/s  0:00:07\n",
      "Installing collected packages: shapely, protobuf, grpcio, google-crc32c, docstring_parser, proto-plus, googleapis-common-protos, google-resumable-media, grpcio-status, google-api-core, grpc-google-iam-v1, google-cloud-core, litellm, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "  Attempting uninstall: protobuf\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "    Found existing installation: protobuf 5.29.3\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "    Uninstalling protobuf-5.29.3:\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "      Successfully uninstalled protobuf-5.29.3\n",
      "   ----------------------------------------  0/17 [shapely]\n",
      "   -- -------------------------------------  1/17 [protobuf]\n",
      "   -- -------------------------------------  1/17 [protobuf]\n",
      "   -- -------------------------------------  1/17 [protobuf]\n",
      "   -- -------------------------------------  1/17 [protobuf]\n",
      "   -- -------------------------------------  1/17 [protobuf]\n",
      "   -- -------------------------------------  1/17 [protobuf]\n",
      "  Attempting uninstall: grpcio\n",
      "   -- -------------------------------------  1/17 [protobuf]\n",
      "    Found existing installation: grpcio 1.67.1\n",
      "   -- -------------------------------------  1/17 [protobuf]\n",
      "   ---- -----------------------------------  2/17 [grpcio]\n",
      "    Uninstalling grpcio-1.67.1:\n",
      "   ---- -----------------------------------  2/17 [grpcio]\n",
      "      Successfully uninstalled grpcio-1.67.1\n",
      "   ---- -----------------------------------  2/17 [grpcio]\n",
      "   ---- -----------------------------------  2/17 [grpcio]\n",
      "   ---- -----------------------------------  2/17 [grpcio]\n",
      "   ---- -----------------------------------  2/17 [grpcio]\n",
      "   ---- -----------------------------------  2/17 [grpcio]\n",
      "   ---- -----------------------------------  2/17 [grpcio]\n",
      "   --------- ------------------------------  4/17 [docstring_parser]\n",
      "   --------- ------------------------------  4/17 [docstring_parser]\n",
      "   ----------- ----------------------------  5/17 [proto-plus]\n",
      "   ----------- ----------------------------  5/17 [proto-plus]\n",
      "   -------------- -------------------------  6/17 [googleapis-common-protos]\n",
      "   -------------- -------------------------  6/17 [googleapis-common-protos]\n",
      "   -------------- -------------------------  6/17 [googleapis-common-protos]\n",
      "   -------------- -------------------------  6/17 [googleapis-common-protos]\n",
      "   -------------- -------------------------  6/17 [googleapis-common-protos]\n",
      "   -------------- -------------------------  6/17 [googleapis-common-protos]\n",
      "   -------------- -------------------------  6/17 [googleapis-common-protos]\n",
      "   -------------- -------------------------  6/17 [googleapis-common-protos]\n",
      "   -------------- -------------------------  6/17 [googleapis-common-protos]\n",
      "   ---------------- -----------------------  7/17 [google-resumable-media]\n",
      "   ---------------- -----------------------  7/17 [google-resumable-media]\n",
      "   ------------------ ---------------------  8/17 [grpcio-status]\n",
      "   --------------------- ------------------  9/17 [google-api-core]\n",
      "   --------------------- ------------------  9/17 [google-api-core]\n",
      "   --------------------- ------------------  9/17 [google-api-core]\n",
      "   --------------------- ------------------  9/17 [google-api-core]\n",
      "   --------------------- ------------------  9/17 [google-api-core]\n",
      "   --------------------- ------------------  9/17 [google-api-core]\n",
      "   ----------------------- ---------------- 10/17 [grpc-google-iam-v1]\n",
      "   ------------------------- -------------- 11/17 [google-cloud-core]\n",
      "  Attempting uninstall: litellm\n",
      "   ------------------------- -------------- 11/17 [google-cloud-core]\n",
      "    Found existing installation: litellm 1.80.11\n",
      "   ------------------------- -------------- 11/17 [google-cloud-core]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "    Uninstalling litellm-1.80.11:\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "      Successfully uninstalled litellm-1.80.11\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ---------------------------- ----------- 12/17 [litellm]\n",
      "   ------------------------------ --------- 13/17 [google-cloud-storage]\n",
      "   ------------------------------ --------- 13/17 [google-cloud-storage]\n",
      "   ------------------------------ --------- 13/17 [google-cloud-storage]\n",
      "   ------------------------------ --------- 13/17 [google-cloud-storage]\n",
      "   ------------------------------ --------- 13/17 [google-cloud-storage]\n",
      "   ------------------------------ --------- 13/17 [google-cloud-storage]\n",
      "   ------------------------------- ------ 14/17 [google-cloud-resource-manager]\n",
      "   ------------------------------- ------ 14/17 [google-cloud-resource-manager]\n",
      "   ------------------------------- ------ 14/17 [google-cloud-resource-manager]\n",
      "   ------------------------------- ------ 14/17 [google-cloud-resource-manager]\n",
      "   ------------------------------- ------ 14/17 [google-cloud-resource-manager]\n",
      "   ------------------------------- ------ 14/17 [google-cloud-resource-manager]\n",
      "   ------------------------------- ------ 14/17 [google-cloud-resource-manager]\n",
      "   ------------------------------- ------ 14/17 [google-cloud-resource-manager]\n",
      "   ------------------------------- ------ 14/17 [google-cloud-resource-manager]\n",
      "   ----------------------------------- ---- 15/17 [google-cloud-bigquery]\n",
      "   ----------------------------------- ---- 15/17 [google-cloud-bigquery]\n",
      "   ----------------------------------- ---- 15/17 [google-cloud-bigquery]\n",
      "   ----------------------------------- ---- 15/17 [google-cloud-bigquery]\n",
      "   ----------------------------------- ---- 15/17 [google-cloud-bigquery]\n",
      "   ----------------------------------- ---- 15/17 [google-cloud-bigquery]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ------------------------------------- -- 16/17 [google-cloud-aiplatform]\n",
      "   ---------------------------------------- 17/17 [google-cloud-aiplatform]\n",
      "\n",
      "Successfully installed docstring_parser-0.17.0 google-api-core-2.28.1 google-cloud-aiplatform-1.132.0 google-cloud-bigquery-3.39.0 google-cloud-core-2.5.0 google-cloud-resource-manager-1.15.0 google-cloud-storage-3.7.0 google-crc32c-1.8.0 google-resumable-media-2.8.0 googleapis-common-protos-1.72.0 grpc-google-iam-v1-0.14.3 grpcio-1.76.0 grpcio-status-1.76.0 litellm-1.80.5 proto-plus-1.27.0 protobuf-6.33.2 shapely-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'restart' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"semantic-router[google]\" && restart kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aa25064-e400-48db-8b05-422ad66e727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting semantic-router==0.1.2 (from semantic-router[google]==0.1.2)\n",
      "  Downloading semantic_router-0.1.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: aiohttp<4,>=3.10.11 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router==0.1.2->semantic-router[google]==0.1.2) (3.13.2)\n",
      "Collecting aurelio-sdk<0.0.19,>=0.0.18 (from semantic-router==0.1.2->semantic-router[google]==0.1.2)\n",
      "  Downloading aurelio_sdk-0.0.18-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: colorama<0.5,>=0.4.6 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router==0.1.2->semantic-router[google]==0.1.2) (0.4.6)\n",
      "Requirement already satisfied: colorlog<7,>=6.8.0 in d:\\aspire\\conda_dire\\lib\\site-packages (from semantic-router==0.1.2->semantic-router[google]==0.1.2) (6.10.1)\n",
      "Collecting numpy<2,>=1.25.2 (from semantic-router==0.1.2->semantic-router[google]==0.1.2)\n",
      "  Downloading numpy-1.26.4.tar.gz (15.8 MB)\n",
      "     ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.6/15.8 MB 16.6 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 6.6/15.8 MB 18.4 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 11.3/15.8 MB 19.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 15.2/15.8 MB 19.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 15.8/15.8 MB 18.3 MB/s  0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [21 lines of output]\n",
      "  + D:\\Aspire\\conda_dire\\python.exe C:\\Users\\thefl\\AppData\\Local\\Temp\\pip-install-yxs5_yqj\\numpy_b6305eb930714086be9e82bcca21d8d0\\vendored-meson\\meson\\meson.py setup C:\\Users\\thefl\\AppData\\Local\\Temp\\pip-install-yxs5_yqj\\numpy_b6305eb930714086be9e82bcca21d8d0 C:\\Users\\thefl\\AppData\\Local\\Temp\\pip-install-yxs5_yqj\\numpy_b6305eb930714086be9e82bcca21d8d0\\.mesonpy-qlycacwm -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\thefl\\AppData\\Local\\Temp\\pip-install-yxs5_yqj\\numpy_b6305eb930714086be9e82bcca21d8d0\\.mesonpy-qlycacwm\\meson-python-native-file.ini\n",
      "  The Meson build system\n",
      "  Version: 1.2.99\n",
      "  Source dir: C:\\Users\\thefl\\AppData\\Local\\Temp\\pip-install-yxs5_yqj\\numpy_b6305eb930714086be9e82bcca21d8d0\n",
      "  Build dir: C:\\Users\\thefl\\AppData\\Local\\Temp\\pip-install-yxs5_yqj\\numpy_b6305eb930714086be9e82bcca21d8d0\\.mesonpy-qlycacwm\n",
      "  Build type: native build\n",
      "  Project name: NumPy\n",
      "  Project version: 1.26.4\n",
      "  WARNING: Failed to activate VS environment: Could not parse vswhere.exe output\n",
      "  \n",
      "  ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "  The following exception(s) were encountered:\n",
      "  Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  \n",
      "  A full log can be found at C:\\Users\\thefl\\AppData\\Local\\Temp\\pip-install-yxs5_yqj\\numpy_b6305eb930714086be9e82bcca21d8d0\\.mesonpy-qlycacwm\\meson-logs\\meson-log.txt\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "numpy\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U \"semantic-router[google]==0.1.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006adb7a-4339-4c94-8a20-a058337cd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform vertexai-language-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6d6d45-c4ad-4135-a437-37124997668d",
   "metadata": {},
   "source": [
    "# Setup\n",
    "This agent is implemented using OpenAI's models, but can be used also with self-hosted LLM and embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4327f7-05dc-4be0-a7e0-ac7eb47bbfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model \n",
    "load_dotenv()\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "#TavilySearch\n",
    "# os.environ[\"TAVILY_API_KEY\"] = os.getenv('TAVILY_API_KEY')\n",
    "\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "llm = init_chat_model(\"google_genai:models/gemini-flash-lite-latest\")\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea5674-fb16-4e72-95a2-5df87bcca32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c60be-7291-418d-b2db-ecef5f429d3d",
   "metadata": {},
   "source": [
    "## Data Models Definition\n",
    "In this section, we define the core data structures for our adaptive learning system using Pydantic models. These models ensure type safety and provide clear structure for:\n",
    "- Learning goals and objectives\n",
    "- Checkpoint definitions and tracking\n",
    "- Search queries for dynamic content\n",
    "- Verification of learning progress\n",
    "- Feynman teaching output format\n",
    "- Question generation\n",
    "\n",
    "Each model is designed to capture specific aspects of the learning process while maintaining strict type validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf74ec-9aca-4023-ac71-c095bf68f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Goals(BaseModel):\n",
    "    \"\"\"Structure for defining learning goals\"\"\"\n",
    "    goals: str = Field(None, description=\"Learning goals\")\n",
    "\n",
    "class LearningCheckpoint(BaseModel):\n",
    "    \"\"\"Structure for a single checkpoint\"\"\"\n",
    "    description: str = Field(..., description=\"Main checkpoint description\")\n",
    "    criteria: List[str] = Field(..., description=\"List of success criteria\")\n",
    "    verification: str = Field(..., description=\"How to verify this checkpoint\")\n",
    "\n",
    "class Checkpoints(BaseModel):\n",
    "    \"\"\"Main checkpoints container with index tracking\"\"\"\n",
    "    checkpoints: List[LearningCheckpoint] = Field(\n",
    "        ..., \n",
    "        description=\"List of checkpoints covering foundation, application, and mastery levels\"\n",
    "    )\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"Structure for search query collection\"\"\"\n",
    "    search_queries: list = Field(None, description=\"Search queries for retrieval.\")\n",
    "\n",
    "class LearningVerification(BaseModel):\n",
    "    \"\"\"Structure for verification results\"\"\"\n",
    "    understanding_level: float = Field(..., ge=0, le=1)\n",
    "    feedback: str\n",
    "    suggestions: List[str]\n",
    "    context_alignment: bool\n",
    "\n",
    "class FeynmanTeaching(BaseModel):\n",
    "    \"\"\"Structure for Feynman teaching method\"\"\"\n",
    "    simplified_explanation: str\n",
    "    key_concepts: List[str]\n",
    "    analogies: List[str]\n",
    "\n",
    "class QuestionOutput(BaseModel):\n",
    "    \"\"\"Structure for question generation output\"\"\"\n",
    "    question: str\n",
    "\n",
    "class InContext(BaseModel):\n",
    "    \"\"\"Structure for context verification\"\"\"\n",
    "    is_in_context: str = Field(..., description=\"Yes or No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855943ac-3129-4da0-8bb6-7f180044e90c",
   "metadata": {},
   "source": [
    "## Learning State Definition\n",
    "Here we define the main state for our agent. This state tracks:\n",
    "- The learning topic and goals\n",
    "- Context and search results\n",
    "- Current progress through checkpoints\n",
    "- Verification results and teaching outputs\n",
    "- Current question-answer pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4028e38-ca23-48ab-93ed-f490e2ce4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningState(TypedDict):\n",
    "    topic: str\n",
    "    goals: List[Goals]\n",
    "    context: str\n",
    "    context_chunks: Annotated[list, operator.add]\n",
    "    context_key: str\n",
    "    search_queries: SearchQuery\n",
    "    checkpoints: Checkpoints\n",
    "    verifications: LearningVerification\n",
    "    teachings: FeynmanTeaching\n",
    "    current_checkpoint: int\n",
    "    current_question: QuestionOutput\n",
    "    current_answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d49c1-3107-45aa-aa23-8caf9a4effef",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "The system uses three utility functions:\n",
    "\n",
    "1. `extract_content_from_chunks`: Processes and combines text chunks into coherent content\n",
    "\n",
    "2. `format_checkpoints_as_message`: Converts checkpoint data into prompt format\n",
    "\n",
    "3. `generate_checkpoint_message`: Creates formatted message for context retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667d407-5f13-4095-93da-dd082b1b4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_chunks(chunks):\n",
    "    \"\"\"Extract and combine content from chunks with splits attribute.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk objects that may contain splits attribute\n",
    "        \n",
    "    Returns:\n",
    "        str: Combined content from all chunks joined with newlines\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'splits') and chunk.splits:\n",
    "            chunk_content = ' '.join(chunk.splits)\n",
    "            content.append(chunk_content)\n",
    "    \n",
    "    return '\\n'.join(content)\n",
    "\n",
    "def format_checkpoints_as_message(checkpoints: Checkpoints) -> str:\n",
    "    \"\"\"Convert Checkpoints object to a formatted string for the message.\n",
    "    \n",
    "    Args:\n",
    "        checkpoints (Checkpoints): Checkpoints object containing learning checkpoints\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string containing numbered checkpoints with descriptions and criteria\n",
    "    \"\"\"\n",
    "    message = \"Here are the learning checkpoints:\\n\\n\"\n",
    "    for i, checkpoint in enumerate(checkpoints.checkpoints, 1):\n",
    "        message += f\"Checkpoint {i}:\\n\"\n",
    "        message += f\"Description: {checkpoint.description}\\n\"\n",
    "        message += \"Success Criteria:\\n\"\n",
    "        for criterion in checkpoint.criteria:\n",
    "            message += f\"- {criterion}\\n\"\n",
    "    return message\n",
    "\n",
    "def generate_checkpoint_message(checks: List[LearningCheckpoint]) -> HumanMessage:\n",
    "    \"\"\"Generate a formatted message for learning checkpoints that need context.\n",
    "    \n",
    "    Args:\n",
    "        checks (List[LearningCheckpoint]): List of learning checkpoint objects\n",
    "        \n",
    "    Returns:\n",
    "        HumanMessage: Formatted message containing checkpoint descriptions, criteria and \n",
    "                     verification methods, ready for context search\n",
    "    \"\"\"\n",
    "    formatted_checks = []\n",
    "    \n",
    "    for check in checks:\n",
    "        checkpoint_text = f\"\"\"\n",
    "        Description: {check.description}\n",
    "        Success Criteria:\n",
    "        {chr(10).join(f'- {criterion}' for criterion in check.criteria)}\n",
    "        Verification Method: {check.verification}\n",
    "        \"\"\"\n",
    "        formatted_checks.append(checkpoint_text)\n",
    "    \n",
    "    all_checks = \"\\n---\\n\".join(formatted_checks)\n",
    "    \n",
    "    checkpoints_message = HumanMessage(content=f\"\"\"The following learning checkpoints need additional context:\n",
    "        {all_checks}\n",
    "        \n",
    "        Please generate search queries to find relevant information.\"\"\")\n",
    "    \n",
    "    return checkpoints_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed58db0-afa2-4591-90f0-51d7027f36e3",
   "metadata": {},
   "source": [
    "## Prompt Configuration\n",
    "Here we define the core instruction prompts for our LLM. Each message serves a specific purpose in the learning process:\n",
    "\n",
    "1. `learning_checkpoints_generator`: Creates structured learning milestones with clear criteria\n",
    "2. `checkpoint_based_query_generator`: Generates targeted search queries for content retrieval\n",
    "3. `question_generator`: Creates verification questions aligned with checkpoints\n",
    "4. `answer_verifier`: Evaluates learner responses against success criteria\n",
    "5. `feynman_teacher`: Crafts simplified explanations using the Feynman technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803f4dd-b580-40de-95f6-a973927c74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_checkpoints_generator = SystemMessage(content=\"\"\"You will be given a learning topic title and learning objectives.\n",
    "Your goal is to generate clear learning checkpoints that will help verify understanding and progress through the topic.\n",
    "The output should be in the following dictionary structure:\n",
    "checkpoint \n",
    "-> description (level checkpoint description)\n",
    "-> criteria\n",
    "-> verification (How to verify this checkpoint (Feynman Methods))\n",
    "Requirements for each checkpoint:\n",
    "- Description should be clear and concise\n",
    "- Criteria should be specific and measurable (3-5 items)\n",
    "- Verification method should be practical and appropriate for the level\n",
    "- Verification will be checked by language model, so it must by in natural language\n",
    "- All elements should align with the learning objectives\n",
    "- Use action verbs and clear language\n",
    "Ensure all checkpoints progress logically from foundation to mastery.\n",
    "IMPORTANT - ANSWER ONLY 3 CHECKPOINTS\"\"\")\n",
    "\n",
    "checkpoint_based_query_generator = SystemMessage(content=\"\"\"You will be given learning checkpoints for a topic.\n",
    "Your goal is to generate search queries that will retrieve content matching each checkpoint's requirements from retrieval systems or web search.\n",
    "Follow these steps:\n",
    "1. Analyze each learning checkpoint carefully\n",
    "2. For each checkpoint, generate ONE targeted search query that will retrieve:\n",
    "   - Content for checkpoint verification\"\"\")\n",
    "\n",
    "validate_context = SystemMessage(content=\"\"\"You will be given a learning criteria and context.\n",
    "Check if the the criteria could be answered using the context.\n",
    "Always answer YES or NO\"\"\")\n",
    "\n",
    "question_generator = SystemMessage(content=\"\"\"You will be given a checkpoint description, success criteria, and verification method.\n",
    "Your goal is to generate an appropriate question that aligns with the checkpoint's verification requirements.\n",
    "The question should:\n",
    "1. Follow the specified verification method\n",
    "2. Cover all success criteria\n",
    "3. Encourage demonstration of understanding\n",
    "4. Be clear and specific\n",
    "Output should be a single, well-formulated question that effectively tests the checkpoint's learning objectives.\"\"\")\n",
    "\n",
    "answer_verifier = SystemMessage(content=\"\"\"You will be given a student's answer, question, checkpoint details, and relevant context.\n",
    "Your goal is to analyze the answer against the checkpoint criteria and provided context.\n",
    "Analyze considering:\n",
    "1. Alignment with verification method specified\n",
    "2. Coverage of all success criteria\n",
    "3. Use of relevant concepts from context\n",
    "4. Depth and accuracy of understanding\n",
    "Output should include:\n",
    "- understanding_level: float between 0 and 1\n",
    "- feedback: detailed explanation of the assessment\n",
    "- suggestions: list of specific improvements\n",
    "- context_alignment: boolean indicating if the answer aligns with provided context\"\"\")\n",
    "\n",
    "feynman_teacher = SystemMessage(content=\"\"\"You will be given verification results, checkpoint criteria, and learning context.\n",
    "Your goal is to create a Feynman-style teaching explanation for concepts that need reinforcement.\n",
    "The explanation should include:\n",
    "1. Simplified explanation without technical jargon\n",
    "2. Concrete, relatable analogies\n",
    "3. Key concepts to remember\n",
    "Output should follow the Feynman technique:\n",
    "- simplified_explanation: clear, jargon-free explanation\n",
    "- key_concepts: list of essential points\n",
    "- analogies: list of relevant, concrete comparisons\n",
    "Focus on making complex ideas accessible and memorable.\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438fbce1-78c8-449a-9494-491399c53a23",
   "metadata": {},
   "source": [
    "## Context Storage\n",
    "The `ContextStore` class manages context chunks and embeddings in memory, optimizing token usage by allowing access to only relevant context during answer verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228f761-da38-4d23-9add-b5178698a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextStore:\n",
    "    \"\"\"Store for managing context chunks and their embeddings in memory.\n",
    "    \n",
    "    A class that provides storage and retrieval of context data using an in-memory store.\n",
    "    Each context entry consists of context chunks and their corresponding embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize ContextStore with an empty in-memory store.\"\"\"\n",
    "        self.store = InMemoryStore()\n",
    "        \n",
    "    def save_context(self, context_chunks: list, embeddings: list, key: str = None):\n",
    "        \"\"\"Save context chunks and their embeddings to the store.\n",
    "        \n",
    "        Args:\n",
    "            context_chunks (list): List of context chunk objects\n",
    "            embeddings (list): List of corresponding embeddings for the chunks\n",
    "            key (str, optional): Custom key for storing the context. Defaults to None,\n",
    "                               in which case a UUID is generated.\n",
    "            \n",
    "        Returns:\n",
    "            str: The key used to store the context\n",
    "        \"\"\"\n",
    "        namespace = (\"context\",)\n",
    "        \n",
    "        if key is None:\n",
    "            key = str(uuid.uuid4())\n",
    "            \n",
    "        value = {\n",
    "            \"chunks\": context_chunks,\n",
    "            \"embeddings\": embeddings\n",
    "        }\n",
    "        \n",
    "        self.store.put(namespace, key, value)\n",
    "        return key\n",
    "        \n",
    "    def get_context(self, context_key: str):\n",
    "        \"\"\"Retrieve context data from the store using a key.\n",
    "        \n",
    "        Args:\n",
    "            context_key (str): The key used to store the context\n",
    "            \n",
    "        Returns:\n",
    "            dict: The stored context value containing chunks and embeddings\n",
    "        \"\"\"\n",
    "        namespace = (\"context\",)\n",
    "        memory = self.store.get(namespace, context_key)\n",
    "        return memory.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bab2b6-c2c3-43ec-a705-ac7e97fe7a4a",
   "metadata": {},
   "source": [
    "## Core Learning System Functions\n",
    "The learning system is powered by eight main functions that process and update the `LearningState`:\n",
    "\n",
    "### Content Generation and Processing\n",
    "1. `generate_checkpoints`: Creates learning milestones from topic and goals\n",
    "2. `generate_query`: Formulates checkpoint-based search queries\n",
    "3. `search_web`: Retrieves content via Tavilysearch\n",
    "5. `chunk_context`: Segments learning materials\n",
    "6. `context_validation`: Ensures context meets checkpoint requirements\n",
    "\n",
    "### Learning Verification and Support\n",
    "6. `generate_question`: Creates verification questions\n",
    "7. `verify_answer`: Evaluates against checkpoint criteria\n",
    "8. `teach_concept`: Provides Feynman-style explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097d06e-cf59-476e-9200-247cadba83aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(state: LearningState):\n",
    "    \"\"\"Generates search queries based on learning checkpoints from current state.\"\"\"\n",
    "    structured_llm = llm.with_structured_output(SearchQuery) \n",
    "    checkpoints_message = HumanMessage(content=format_checkpoints_as_message(state['checkpoints']))  \n",
    "    messages = [checkpoint_based_query_generator, checkpoints_message]\n",
    "    search_queries = structured_llm.invoke(messages)\n",
    "    return {\"search_queries\": search_queries}\n",
    "\n",
    "def search_web(state: LearningState):\n",
    "    \"\"\"Retrieves and processes web search results based on search queries.\"\"\"\n",
    "    search_queries = state[\"search_queries\"].search_queries\n",
    "    \n",
    "    all_search_docs = []\n",
    "    for query in search_queries:\n",
    "        search_docs = tavily_search.invoke(query)\n",
    "        all_search_docs.extend(search_docs)\n",
    "    \n",
    "    formatted_search_docs = [\n",
    "        f'Context: {doc[\"content\"]}\\n Source: {doc[\"url\"]}\\n'\n",
    "        for doc in all_search_docs\n",
    "    ]\n",
    "\n",
    "    chunk_embeddings = embeddings.embed_documents(formatted_search_docs)\n",
    "    context_key = context_store.save_context(\n",
    "        formatted_search_docs,\n",
    "        chunk_embeddings,\n",
    "        key=state.get('context_key')\n",
    "    )\n",
    "    \n",
    "    return {\"context_chunks\": formatted_search_docs}\n",
    "\n",
    "def generate_checkpoints(state: LearningState):\n",
    "    \"\"\"Creates learning checkpoints based on given topic and goals.\"\"\"\n",
    "    structured_llm = llm.with_structured_output(Checkpoints)\n",
    "    messages = [\n",
    "        learning_checkpoints_generator,\n",
    "        HumanMessage(content=f\"Topic: {state['topic']}\\nGoals: {', '.join(str(goal) for goal in state['goals'])}\")\n",
    "    ]\n",
    "    checkpoints = structured_llm.invoke(messages)\n",
    "    return {\"checkpoints\": checkpoints}\n",
    "\n",
    "def chunk_context(state: LearningState):\n",
    "    \"\"\"Splits context into manageable chunks and generates their embeddings.\"\"\"\n",
    "    encoder = OpenAIEncoder(name=\"text-embedding-3-large\")\n",
    "    chunker = StatisticalChunker(\n",
    "        encoder=encoder,\n",
    "        min_split_tokens=128,\n",
    "        max_split_tokens=512\n",
    "    )\n",
    "    \n",
    "    chunks = chunker([state['context']])\n",
    "    content = []\n",
    "    for chunk in chunks:\n",
    "        content.append(extract_content_from_chunks(chunk))\n",
    "\n",
    "    chunk_embeddings = embeddings.embed_documents(content)\n",
    "    context_key = context_store.save_context(\n",
    "        content,\n",
    "        chunk_embeddings,\n",
    "        key=state.get('context_key')\n",
    "    )\n",
    "    return {\"context_chunks\": content, \"context_key\": context_key}\n",
    "\n",
    "def context_validation(state: LearningState):\n",
    "    \"\"\"Validates context coverage against checkpoint criteria using stored embeddings.\"\"\"\n",
    "    context = context_store.get_context(state['context_key'])\n",
    "    chunks = context['chunks']\n",
    "    chunk_embeddings = context['embeddings']\n",
    "    \n",
    "    checks = []\n",
    "    structured_llm = llm.with_structured_output(InContext)\n",
    "    \n",
    "    for checkpoint in state['checkpoints'].checkpoints:\n",
    "        query = embeddings.embed_query(checkpoint.verification)\n",
    "        \n",
    "        similarities = cosine_similarity([query], chunk_embeddings)[0]\n",
    "        top_3_indices = sorted(range(len(similarities)), \n",
    "                             key=lambda i: similarities[i], \n",
    "                             reverse=True)[:3]\n",
    "        relevant_chunks = [chunks[i] for i in top_3_indices]\n",
    "        \n",
    "        messages = [\n",
    "            validate_context,\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Criteria:\n",
    "            {chr(10).join(f\"- {c}\" for c in checkpoint.criteria)}\n",
    "            \n",
    "            Context:\n",
    "            {chr(10).join(relevant_chunks)}\n",
    "            \"\"\")\n",
    "        ]\n",
    "        \n",
    "        response = structured_llm.invoke(messages)\n",
    "        if response.is_in_context.lower() == \"no\":\n",
    "            checks.append(checkpoint)\n",
    "    \n",
    "    if checks:\n",
    "        structured_llm = llm.with_structured_output(SearchQuery)\n",
    "        checkpoints_message = generate_checkpoint_message(checks)\n",
    "        \n",
    "        messages = [checkpoint_based_query_generator, checkpoints_message]\n",
    "        search_queries = structured_llm.invoke(messages)\n",
    "        return {\"search_queries\": search_queries}\n",
    "    \n",
    "    return {\"search_queries\": None}\n",
    "\n",
    "def generate_question(state: LearningState):\n",
    "    \"\"\"Generates assessment questions based on current checkpoint verification requirements.\"\"\"\n",
    "    structured_llm = llm.with_structured_output(QuestionOutput)\n",
    "    current_checkpoint = state['current_checkpoint']\n",
    "    checkpoint_info = state['checkpoints'].checkpoints[current_checkpoint]\n",
    "    \n",
    "    messages = [\n",
    "        question_generator,\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Checkpoint Description: {checkpoint_info.description}\n",
    "        Success Criteria:\n",
    "        {chr(10).join(f\"- {c}\" for c in checkpoint_info.criteria)}\n",
    "        Verification Method: {checkpoint_info.verification}\n",
    "        \n",
    "        Generate an appropriate verification question.\"\"\")\n",
    "    ]\n",
    "    \n",
    "    question_output = structured_llm.invoke(messages)\n",
    "    return {\"current_question\": question_output.question}\n",
    "\n",
    "def verify_answer(state: LearningState):\n",
    "    \"\"\"Evaluates user answers against checkpoint criteria using relevant context chunks.\"\"\"\n",
    "    structured_llm = llm.with_structured_output(LearningVerification)\n",
    "    current_checkpoint = state['current_checkpoint']\n",
    "    checkpoint_info = state['checkpoints'].checkpoints[current_checkpoint]\n",
    "    \n",
    "    context = context_store.get_context(state['context_key'])\n",
    "    chunks = context['chunks']\n",
    "    chunk_embeddings = context['embeddings']\n",
    "    \n",
    "    query = embeddings.embed_query(checkpoint_info.verification)\n",
    "    \n",
    "    similarities = cosine_similarity([query], chunk_embeddings)[0]\n",
    "    top_3_indices = sorted(range(len(similarities)), \n",
    "                         key=lambda i: similarities[i], \n",
    "                         reverse=True)[:3]\n",
    "    relevant_chunks = [chunks[i] for i in top_3_indices]\n",
    "    \n",
    "    messages = [\n",
    "        answer_verifier,\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Question: {state['current_question']}\n",
    "        Answer: {state['current_answer']}\n",
    "        \n",
    "        Checkpoint Description: {checkpoint_info.description}\n",
    "        Success Criteria:\n",
    "        {chr(10).join(f\"- {c}\" for c in checkpoint_info.criteria)}\n",
    "        Verification Method: {checkpoint_info.verification}\n",
    "        \n",
    "        Context:\n",
    "        {chr(10).join(relevant_chunks)}\n",
    "        \n",
    "        Assess the answer.\"\"\")\n",
    "    ]\n",
    "    \n",
    "    verification = structured_llm.invoke(messages)\n",
    "    return {\"verifications\": verification}\n",
    "    \n",
    "def teach_concept(state: LearningState):\n",
    "    \"\"\"Creates simplified Feynman-style explanations for concepts that need reinforcement.\"\"\"\n",
    "    structured_llm = llm.with_structured_output(FeynmanTeaching)\n",
    "    current_checkpoint = state['current_checkpoint']\n",
    "    checkpoint_info = state['checkpoints'].checkpoints[current_checkpoint]\n",
    "    \n",
    "    messages = [\n",
    "        feynman_teacher,\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Criteria: {checkpoint_info.criteria}\n",
    "        Verification: {state['verifications']}\n",
    "        \n",
    "        Context:\n",
    "        {state['context_chunks']}\n",
    "        \n",
    "        Create a Feynman teaching explanation.\"\"\")\n",
    "    ]\n",
    "    \n",
    "    teaching = structured_llm.invoke(messages)\n",
    "    return {\"teachings\": teaching}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db57b9-281e-465f-97e2-9fd10279d8d3",
   "metadata": {},
   "source": [
    "## Helper State Management Functions\n",
    "Here we define two auxiliary functions that manage the learning flow:\n",
    "\n",
    "1. `user_answer`: Placeholder for collecting user responses to verification questions\n",
    "2. `next_checkpoint`: Increments the checkpoint counter to progress through learning milestones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5e79a-d675-476b-b52e-c8ced9d4ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_answer(state: LearningState):\n",
    "    \"\"\"Placeholder for handling user's answer input.\"\"\"\n",
    "    pass\n",
    "\n",
    "def next_checkpoint(state: LearningState):\n",
    "    \"\"\"Advances to the next checkpoint in the learning sequence.\"\"\"\n",
    "    current_checkpoint = state['current_checkpoint'] + 1\n",
    "    return {'current_checkpoint': current_checkpoint}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf396447-7a62-477c-b7ff-73ceb7cd2457",
   "metadata": {},
   "source": [
    "## Routing Logic Functions\n",
    "Four routing functions control the agent's workflow:\n",
    "\n",
    "1. `route_context`: Manages context processing vs. query generation\n",
    "2. `route_verification`: Directs flow based on understanding level (70% threshold)\n",
    "3. `route_teaching`: Handles post-teaching progression\n",
    "4. `route_search`: Back to search if context is irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98026d70-35c1-4efc-bbe0-08178e0953a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_context(state: LearningState):\n",
    "    \"\"\"Determines whether to process existing context or generate new search queries.\"\"\"\n",
    "    if state.get(\"context\"):\n",
    "        return 'chunk_context'\n",
    "    return 'generate_query'\n",
    "\n",
    "def route_verification(state: LearningState):\n",
    "    \"\"\"Determines next step based on verification results and checkpoint progress.\"\"\"\n",
    "    current_checkpoint = state['current_checkpoint']\n",
    "    \n",
    "    if state['verifications'].understanding_level < 0.7:\n",
    "        return 'teach_concept'\n",
    "        \n",
    "    if current_checkpoint + 1 < len(state['checkpoints'].checkpoints):\n",
    "        return 'next_checkpoint'\n",
    "    \n",
    "    return END\n",
    "\n",
    "def route_teaching(state: LearningState):\n",
    "    \"\"\"Routes to next checkpoint or ends session after teaching intervention.\"\"\"\n",
    "    current_checkpoint = state['current_checkpoint']\n",
    "    if current_checkpoint + 1 < len(state['checkpoints'].checkpoints):\n",
    "        return 'next_checkpoint'\n",
    "    return END\n",
    "\n",
    "def route_search(state: LearningState):\n",
    "    \"\"\"Directs flow between question generation and web search based on query status.\"\"\"\n",
    "    if state['search_queries'] is None:\n",
    "        return \"generate_question\"\n",
    "    return \"search_web\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e2154b-f06e-45e9-a0d8-fa112623e4ef",
   "metadata": {},
   "source": [
    "## Building the Learning Flow Graph\n",
    "Here we construct the complete graph structure for our adaptive learning system using LangGraph. The graph defines:\n",
    "\n",
    "1. **Node Setup**\n",
    "  - Core processing nodes (generate_query, search_web, chunk_context, context_validation)\n",
    "  - Learning management nodes (generate_checkpoints, generate_question)\n",
    "  - Interactive nodes (user_answer)\n",
    "  - Evaluation nodes (verify_answer, teach_concept)\n",
    "\n",
    "2. **Flow Definition**\n",
    "  - Starting point (generate_checkpoints)\n",
    "  - Conditional paths based on context availability and verification results\n",
    "  - Interactive breaks for user input\n",
    "\n",
    "3. **Graph Configuration**\n",
    "  - Uses MemorySaver for persistence\n",
    "  - Uses ContextStore to keep embeddings\n",
    "  - Includes human in the loop strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d42bb27-9e0b-4fe6-9bdf-3f2a289ec4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = StateGraph(LearningState)\n",
    "memory = MemorySaver()\n",
    "context_store = ContextStore()\n",
    "\n",
    "searcher.add_node(\"generate_query\", generate_query)\n",
    "searcher.add_node(\"search_web\", search_web)\n",
    "searcher.add_node(\"chunk_context\", chunk_context)\n",
    "searcher.add_node(\"context_validation\", context_validation)\n",
    "searcher.add_node(\"generate_checkpoints\", generate_checkpoints)\n",
    "searcher.add_node(\"generate_question\", generate_question)\n",
    "searcher.add_node(\"next_checkpoint\", next_checkpoint)\n",
    "searcher.add_node(\"user_answer\", user_answer)\n",
    "searcher.add_node(\"verify_answer\", verify_answer)\n",
    "searcher.add_node(\"teach_concept\", teach_concept)\n",
    "\n",
    "# Flow\n",
    "searcher.add_edge(START, \"generate_checkpoints\")\n",
    "searcher.add_conditional_edges('generate_checkpoints', route_context,['chunk_context', 'generate_query'])\n",
    "searcher.add_edge(\"generate_query\", \"search_web\")\n",
    "searcher.add_edge(\"search_web\", \"generate_question\")\n",
    "searcher.add_edge(\"chunk_context\", 'context_validation')\n",
    "searcher.add_conditional_edges('context_validation', route_search,['search_web', 'generate_question'])\n",
    "\n",
    "searcher.add_edge(\"generate_question\", \"user_answer\")\n",
    "searcher.add_edge(\"user_answer\", \"verify_answer\")\n",
    "searcher.add_conditional_edges(\n",
    "    \"verify_answer\",\n",
    "    route_verification,\n",
    "    {\n",
    "        \"next_checkpoint\": \"next_checkpoint\",\n",
    "        \"teach_concept\": \"teach_concept\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "searcher.add_conditional_edges(\n",
    "    \"teach_concept\",\n",
    "    route_teaching,\n",
    "    {\n",
    "        \"next_checkpoint\": \"next_checkpoint\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "searcher.add_edge(\"next_checkpoint\", \"generate_question\")\n",
    "\n",
    "\n",
    "\n",
    "graph = searcher.compile(interrupt_after=[\"generate_checkpoints\"], interrupt_before=[\"user_answer\"], checkpointer=memory)\n",
    "\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98346036-35c7-497a-bb94-e415b4f6f230",
   "metadata": {},
   "source": [
    "# Agent Use Case - Learn Anemia from own note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e42dcf9-db91-450e-9c6f-f6f7d120d5ce",
   "metadata": {},
   "source": [
    "## Pretty print helper functions\n",
    "\n",
    "Helper functions to improve output readability and example visibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b9dd9-8d43-49c8-9c7e-066c5e71e810",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_checkpoints(event):\n",
    "    \"\"\"Pretty print checkpoints information with improved visual formatting\"\"\"\n",
    "    checkpoints = event.get('checkpoints', '')\n",
    "    if checkpoints:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\" LEARNING CHECKPOINTS OVERVIEW\".center(80))\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "        for i, checkpoint in enumerate(checkpoints.checkpoints, 1):\n",
    "            # Checkpoint header with number\n",
    "            print(f\" CHECKPOINT #{i}\".center(80))\n",
    "            print(\"\" * 80 + \"\\n\")\n",
    "            \n",
    "            # Description section with text wrapping\n",
    "            print(\" Description:\")\n",
    "            print(\"\" * 40)\n",
    "            words = checkpoint.description.split()\n",
    "            current_line = []\n",
    "            current_length = 0\n",
    "            \n",
    "            for word in words:\n",
    "                if current_length + len(word) + 1 <= 70:\n",
    "                    current_line.append(word)\n",
    "                    current_length += len(word) + 1\n",
    "                else:\n",
    "                    print(f\"  {' '.join(current_line)}\")\n",
    "                    current_line = [word]\n",
    "                    current_length = len(word)\n",
    "            \n",
    "            if current_line:\n",
    "                print(f\"  {' '.join(current_line)}\")\n",
    "            print()\n",
    "            \n",
    "            # Success Criteria section\n",
    "            print(\" Success Criteria:\")\n",
    "            print(\"\" * 40)\n",
    "            for j, criterion in enumerate(checkpoint.criteria, 1):\n",
    "                # Wrap each criterion text\n",
    "                words = criterion.split()\n",
    "                current_line = []\n",
    "                current_length = 0\n",
    "                first_line = True\n",
    "                \n",
    "                for word in words:\n",
    "                    if current_length + len(word) + 1 <= 66:  # Shorter width to account for numbering\n",
    "                        current_line.append(word)\n",
    "                        current_length += len(word) + 1\n",
    "                    else:\n",
    "                        if first_line:\n",
    "                            print(f\"  {j}. {' '.join(current_line)}\")\n",
    "                            first_line = False\n",
    "                        else:\n",
    "                            print(f\"     {' '.join(current_line)}\")\n",
    "                        current_line = [word]\n",
    "                        current_length = len(word)\n",
    "                \n",
    "                if current_line:\n",
    "                    if first_line:\n",
    "                        print(f\"  {j}. {' '.join(current_line)}\")\n",
    "                    else:\n",
    "                        print(f\"     {' '.join(current_line)}\")\n",
    "            print()\n",
    "            \n",
    "            # Verification Method section\n",
    "            print(\" Verification Method:\")\n",
    "            print(\"\" * 40)\n",
    "            words = checkpoint.verification.split()\n",
    "            current_line = []\n",
    "            current_length = 0\n",
    "            \n",
    "            for word in words:\n",
    "                if current_length + len(word) + 1 <= 70:\n",
    "                    current_line.append(word)\n",
    "                    current_length += len(word) + 1\n",
    "                else:\n",
    "                    print(f\"  {' '.join(current_line)}\")\n",
    "                    current_line = [word]\n",
    "                    current_length = len(word)\n",
    "            \n",
    "            if current_line:\n",
    "                print(f\"  {' '.join(current_line)}\")\n",
    "            print()\n",
    "            \n",
    "            # Separator between checkpoints\n",
    "            if i < len(checkpoints.checkpoints):\n",
    "                print(\"~\" * 80 + \"\\n\")\n",
    "        \n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "def print_verification_results(event):\n",
    "    \"\"\"Pretty print verification results with improved formatting\"\"\"\n",
    "    verifications = event.get('verifications', '')\n",
    "    if verifications:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\" VERIFICATION RESULTS\".center(50))\n",
    "        print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        # Understanding Level with visual bar\n",
    "        understanding = verifications.understanding_level\n",
    "        bar_length = 20\n",
    "        filled_length = int(understanding * bar_length)\n",
    "        bar = \"\" * filled_length + \"\" * (bar_length - filled_length)\n",
    "        \n",
    "        print(f\" Understanding Level: [{bar}] {understanding * 100:.1f}%\\n\")\n",
    "        \n",
    "        # Feedback section\n",
    "        print(\" Feedback:\")\n",
    "        print(f\"{verifications.feedback}\\n\")\n",
    "        \n",
    "        # Suggestions section\n",
    "        print(\" Suggestions:\")\n",
    "        for i, suggestion in enumerate(verifications.suggestions, 1):\n",
    "            print(f\"  {i}. {suggestion}\")\n",
    "        print()\n",
    "        \n",
    "        # Context Alignment\n",
    "        print(\" Context Alignment:\")\n",
    "        print(f\"{verifications.context_alignment}\\n\")\n",
    "        \n",
    "        print(\"-\" * 50 + \"\\n\")\n",
    "def print_teaching_results(event):\n",
    "    \"\"\"Pretty print Feynman teaching results with improved formatting\"\"\"\n",
    "    teachings = event.get('teachings', '')\n",
    "    if teachings:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\" FEYNMAN TEACHING EXPLANATION\".center(70))\n",
    "        print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "        # Simplified Explanation section\n",
    "        print(\" SIMPLIFIED EXPLANATION:\")\n",
    "        print(\"\" * 30)\n",
    "        # Split explanation into paragraphs for better readability\n",
    "        paragraphs = teachings.simplified_explanation.split('\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            # Wrap text at 60 characters for better readability\n",
    "            words = paragraph.split()\n",
    "            lines = []\n",
    "            current_line = []\n",
    "            current_length = 0\n",
    "            \n",
    "            for word in words:\n",
    "                if current_length + len(word) + 1 <= 60:\n",
    "                    current_line.append(word)\n",
    "                    current_length += len(word) + 1\n",
    "                else:\n",
    "                    lines.append(' '.join(current_line))\n",
    "                    current_line = [word]\n",
    "                    current_length = len(word)\n",
    "            \n",
    "            if current_line:\n",
    "                lines.append(' '.join(current_line))\n",
    "            \n",
    "            for line in lines:\n",
    "                print(f\"{line}\")\n",
    "            print()\n",
    "        \n",
    "        # Key Concepts section\n",
    "        print(\" KEY CONCEPTS:\")\n",
    "        print(\"\" * 30)\n",
    "        for i, concept in enumerate(teachings.key_concepts, 1):\n",
    "            print(f\"  {i}. {concept}\")\n",
    "        print()\n",
    "        \n",
    "        # Analogies section\n",
    "        print(\" ANALOGIES & EXAMPLES:\")\n",
    "        print(\"\" * 30)\n",
    "        for i, analogy in enumerate(teachings.analogies, 1):\n",
    "            print(f\"  {i}. {analogy}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644c25e-96d3-4a82-b948-a4d7457afe34",
   "metadata": {},
   "source": [
    "## Example School Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3a5c4-beaf-4111-afc9-f30354115497",
   "metadata": {},
   "outputs": [],
   "source": [
    "note = \"\"\"Anemia: A Comprehensive Overview\n",
    "Definition\n",
    "Anemia is a medical condition characterized by a decrease in the total number of red blood cells (RBCs) or hemoglobin in the blood. This reduction leads to a diminished ability to carry oxygen to the body's tissues, affecting overall body function and health.\n",
    "Blood Components and Their Role\n",
    "Red blood cells, also known as erythrocytes, are fundamental components of blood that carry oxygen throughout the body. These cells contain hemoglobin, an iron-containing protein that gives blood its characteristic red color and is responsible for oxygen transport. The typical lifespan of a red blood cell is approximately 120 days, after which it must be replaced by new cells produced in the bone marrow.\n",
    "Types of Anemia\n",
    "Iron Deficiency Anemia represents the most prevalent form of anemia worldwide. It occurs due to insufficient iron intake or absorption, particularly affecting pregnant women, growing children, menstruating women, and individuals with poor nutritional intake.\n",
    "Vitamin Deficiency Anemia develops when the body lacks sufficient amounts of vitamin B12 or folate (vitamin B9). This deficiency can stem from dietary inadequacies or problems with nutrient absorption in the digestive system.\n",
    "Aplastic Anemia, though rare, presents a serious condition where the bone marrow fails to produce adequate blood cells. This form can be either inherited through genetic factors or acquired through various environmental causes or medical conditions.\n",
    "Hemolytic Anemia occurs when red blood cells are destroyed at a rate faster than the body can replace them. This condition may be inherited through genetic factors or acquired through various external causes.\n",
    "Clinical Manifestations\n",
    "Anemia manifests through various symptoms including persistent fatigue and weakness. Patients often present with pale or yellowish skin, experience shortness of breath, and may suffer from dizziness. Additional symptoms include irregular heartbeat patterns, frequent headaches, cold extremities, and occasional chest pain.\n",
    "Diagnostic Approach\n",
    "Diagnosis begins with a thorough physical examination by a healthcare provider. Blood tests form the cornerstone of diagnosis, including a Complete Blood Count (CBC), assessment of iron levels, vitamin B12 measurement, and folate level determination. These tests help identify the specific type of anemia and guide appropriate treatment.\n",
    "Treatment Strategies\n",
    "Dietary modification serves as a fundamental treatment approach. This involves increasing consumption of iron-rich foods such as red meat, dark leafy vegetables, legumes, and iron-fortified cereals.\n",
    "Supplementation often proves necessary and may include iron supplements, vitamin B12, or folic acid, depending on the underlying cause of anemia.\n",
    "Medical interventions become necessary in severe cases. Blood transfusions may be required for severe anemia, while bone marrow transplantation might be considered for cases of aplastic anemia.\n",
    "Preventive Measures\n",
    "Prevention centers on maintaining a balanced diet rich in essential nutrients, particularly iron, vitamin B12, folate, and vitamin C, which enhances iron absorption. Regular medical check-ups allow for early detection and intervention.\n",
    "Certain populations require special attention regarding prevention. These include pregnant women, menstruating women, growing children, individuals following vegetarian or vegan diets, and athletes who may have increased nutritional demands.\n",
    "Potential Complications\n",
    "Untreated anemia can lead to several serious complications. These include severe fatigue that impacts daily activities, complications during pregnancy, cardiovascular problems, depression, and cognitive difficulties that may affect work or school performance.\n",
    "Clinical Significance\n",
    "Anemia often serves as an indicator of other underlying medical conditions. Therefore, early detection and appropriate treatment prove crucial for optimal outcomes. Different forms of anemia require specific treatment approaches, and regular monitoring may be necessary to ensure treatment effectiveness.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d6d38-d036-48c3-a819-442e65a07a23",
   "metadata": {},
   "source": [
    "## Initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dec5ad-f0bf-493f-8e72-fe0409b5c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_input = {\n",
    "    \"topic\": \"Anemia\",\n",
    "    'goals': ['Im medical student, i want to master the diagnosis of Anemia'],\n",
    "    'context': note,\n",
    "    'current_checkpoint': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2dfe81-c2da-47cc-b209-a0fbe611b493",
   "metadata": {},
   "source": [
    "## Generate learning checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ab6f4-0fae-48a4-8c80-620d5c9e3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"20\"}}\n",
    "\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    print_checkpoints(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b3852-7d5e-4583-af32-21bdd77ea727",
   "metadata": {},
   "source": [
    "## Widget for checkpoin management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545f08e-2341-49fc-aae6-c49cb983aea5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def create_checkpoint_editor(checkpoints_model: Checkpoints):\n",
    "    \"\"\"\n",
    "    Creates an interactive checkpoint editor using a Pydantic model.\n",
    "    \n",
    "    Args:\n",
    "        checkpoints_model: Pydantic model of Checkpoints class\n",
    "    \"\"\"\n",
    "    # Convert to list of dictionaries for easier editing\n",
    "    checkpoints = [cp.model_dump() for cp in checkpoints_model.checkpoints]\n",
    "    checkpoints_widgets = []\n",
    "    accepted_checkpoints = []\n",
    "    \n",
    "    def create_criterion_widget(checkpoint_index: int, criterion_value: str = \"\", criterion_index: int = None):\n",
    "        \"\"\"Creates a widget for a single criterion with a delete button\"\"\"\n",
    "        criterion_container = widgets.HBox([\n",
    "            widgets.Text(\n",
    "                value=criterion_value,\n",
    "                description=f'{criterion_index + 1}.' if criterion_index is not None else 'New',\n",
    "                layout=widgets.Layout(width='85%')\n",
    "            ),\n",
    "            widgets.Button(\n",
    "                description='Delete',\n",
    "                button_style='danger',\n",
    "                layout=widgets.Layout(width='15%')\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        def on_criterion_change(change):\n",
    "            nonlocal criterion_index\n",
    "            if criterion_index is not None:\n",
    "                checkpoints[checkpoint_index]['criteria'][criterion_index] = change['new']\n",
    "        \n",
    "        def remove_criterion(b):\n",
    "            if criterion_index is not None:\n",
    "                checkpoints[checkpoint_index]['criteria'].pop(criterion_index)\n",
    "                update_checkpoint_widget(checkpoint_index)\n",
    "        \n",
    "        criterion_container.children[0].observe(on_criterion_change, names='value')\n",
    "        criterion_container.children[1].on_click(remove_criterion)\n",
    "        \n",
    "        return criterion_container\n",
    "    \n",
    "    def create_checkpoint_widget(checkpoint: dict, index: int):\n",
    "        \"\"\"Creates a widget for a single checkpoint\"\"\"\n",
    "        \n",
    "        def on_accept_change(change):\n",
    "            if change['new']:\n",
    "                accepted_checkpoints.append(index)\n",
    "            else:\n",
    "                if index in accepted_checkpoints:\n",
    "                    accepted_checkpoints.remove(index)\n",
    "        \n",
    "        def on_description_change(change):\n",
    "            checkpoints[index]['description'] = change['new']\n",
    "        \n",
    "        def on_verification_change(change):\n",
    "            checkpoints[index]['verification'] = change['new']\n",
    "        \n",
    "        def add_criterion(b):\n",
    "            checkpoints[index]['criteria'].append(\"\")\n",
    "            update_checkpoint_widget(index)\n",
    "        \n",
    "        def remove_checkpoint(b):\n",
    "            checkpoints.pop(index)\n",
    "            update_all_checkpoints()\n",
    "        \n",
    "        # Header with checkbox and delete button\n",
    "        header = widgets.HBox([\n",
    "            widgets.HTML(f'<h3 style=\"margin: 0;\">Checkpoint {index + 1}</h3>'),\n",
    "            widgets.Checkbox(\n",
    "                value=False,\n",
    "                description='Accept',\n",
    "                indent=False,\n",
    "                layout=widgets.Layout(margin='5px 0 0 20px')\n",
    "            ),\n",
    "            widgets.Button(\n",
    "                description='Delete checkpoint',\n",
    "                button_style='danger',\n",
    "                layout=widgets.Layout(margin='0 0 0 20px')\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Description\n",
    "        description = widgets.Textarea(\n",
    "            value=checkpoint['description'],\n",
    "            description='Description:',\n",
    "            layout=widgets.Layout(width='95%', height='60px')\n",
    "        )\n",
    "        \n",
    "        # Criteria\n",
    "        criteria_label = widgets.HTML('<b>Criteria:</b>')\n",
    "        criteria_container = widgets.VBox([\n",
    "            create_criterion_widget(index, criterion, i)\n",
    "            for i, criterion in enumerate(checkpoint['criteria'])\n",
    "        ])\n",
    "        \n",
    "        # Add criterion button\n",
    "        add_criterion_btn = widgets.Button(\n",
    "            description='Add criterion',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(margin='10px 0')\n",
    "        )\n",
    "        \n",
    "        # Verification\n",
    "        verification = widgets.Textarea(\n",
    "            value=checkpoint['verification'],\n",
    "            description='Verification:',\n",
    "            layout=widgets.Layout(width='95%', height='60px', margin='10px 0')\n",
    "        )\n",
    "        \n",
    "        separator = widgets.HTML('<hr style=\"margin: 20px 0;\">')\n",
    "        \n",
    "        # Combine all elements\n",
    "        checkpoint_widget = widgets.VBox([\n",
    "            header,\n",
    "            description,\n",
    "            criteria_label,\n",
    "            criteria_container,\n",
    "            add_criterion_btn,\n",
    "            verification,\n",
    "            separator\n",
    "        ])\n",
    "        \n",
    "        # Add observers and handlers\n",
    "        header.children[1].observe(on_accept_change, names='value')\n",
    "        header.children[2].on_click(remove_checkpoint)\n",
    "        description.observe(on_description_change, names='value')\n",
    "        verification.observe(on_verification_change, names='value')\n",
    "        add_criterion_btn.on_click(add_criterion)\n",
    "        \n",
    "        return checkpoint_widget\n",
    "    \n",
    "    def update_checkpoint_widget(index: int):\n",
    "        \"\"\"Updates a single checkpoint widget\"\"\"\n",
    "        if 0 <= index < len(checkpoints):\n",
    "            checkpoints_widgets[index] = create_checkpoint_widget(checkpoints[index], index)\n",
    "            update_main_container()\n",
    "    \n",
    "    def update_all_checkpoints():\n",
    "        \"\"\"Updates all checkpoint widgets\"\"\"\n",
    "        nonlocal checkpoints_widgets\n",
    "        checkpoints_widgets = [\n",
    "            create_checkpoint_widget(checkpoint, i)\n",
    "            for i, checkpoint in enumerate(checkpoints)\n",
    "        ]\n",
    "        update_main_container()\n",
    "    \n",
    "    def add_new_checkpoint(b):\n",
    "        \"\"\"Adds a new checkpoint\"\"\"\n",
    "        checkpoints.append({\n",
    "            'description': '',\n",
    "            'criteria': [],\n",
    "            'verification': ''\n",
    "        })\n",
    "        update_all_checkpoints()\n",
    "    \n",
    "    def get_pydantic_model() -> Checkpoints:\n",
    "        \"\"\"Converts the current editor state back to a Pydantic model\"\"\"\n",
    "        return Checkpoints(checkpoints=[\n",
    "            LearningCheckpoint(**checkpoint)\n",
    "            for checkpoint in checkpoints\n",
    "        ])\n",
    "    \n",
    "    # Create initial checkpoint widgets\n",
    "    checkpoints_widgets = [\n",
    "        create_checkpoint_widget(checkpoint, i)\n",
    "        for i, checkpoint in enumerate(checkpoints)\n",
    "    ]\n",
    "    \n",
    "    # Add new checkpoint button\n",
    "    add_checkpoint_btn = widgets.Button(\n",
    "        description='Add checkpoint',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(margin='20px 0')\n",
    "    )\n",
    "    add_checkpoint_btn.on_click(add_new_checkpoint)\n",
    "    \n",
    "    # Main container\n",
    "    main_container = widgets.VBox(\n",
    "        checkpoints_widgets + [add_checkpoint_btn],\n",
    "        layout=widgets.Layout(\n",
    "            padding='20px',\n",
    "            border='1px solid #ddd',\n",
    "            border_radius='5px'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    def update_main_container():\n",
    "        \"\"\"Updates the main container\"\"\"\n",
    "        main_container.children = tuple(checkpoints_widgets + [add_checkpoint_btn])\n",
    "    \n",
    "    # Add method to container to retrieve data later\n",
    "    main_container.get_model = get_pydantic_model\n",
    "    \n",
    "    return main_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9225efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e22b2b-1ef0-40c4-b5d1-5e20055ae03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = event['checkpoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef90d9-fc86-47c8-845b-c81af14a66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = create_checkpoint_editor(checkpoints)\n",
    "display(editor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd2492",
   "metadata": {},
   "source": [
    "## Widget preview - Human in the loop checkpoints modifications\n",
    "\n",
    "![Chiron Widget](../images/chiron_widget.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a2b90-2ee2-43f7-b888-b4e1914798b9",
   "metadata": {},
   "source": [
    "## Upade state with adjusted checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186311ae-96e5-4164-8900-e6e6c05592ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model = editor.get_model()\n",
    "graph.update_state(thread, {\"checkpoints\": updated_model}, as_node=\"generate_checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11661cd6-866b-4521-820b-ad8a013b9e38",
   "metadata": {},
   "source": [
    "## Run agent with input after printing question\n",
    "\n",
    "example answer:\n",
    "Anemia is a medical condition characterized by a deficiency in the number or quality of red blood cells (RBCs) or a reduced amount of hemoglobin in these cells, which impairs the blood's ability to carry oxygen to the bodys tissues. In clinical practice, anemia is significant because it can lead to symptoms such as fatigue, weakness, dizziness, and shortness of breath, which may severely impact quality of life and, in severe cases, lead to life-threatening complications.\n",
    "\n",
    "Types of Anemia and Their Causes\n",
    "Iron Deficiency Anemia: This is the most common type of anemia, often caused by insufficient iron intake, blood loss (e.g., heavy menstruation or gastrointestinal bleeding), or poor iron absorption (due to conditions like celiac disease). Iron is essential for hemoglobin production, and a deficiency in iron leads to decreased hemoglobin levels, reducing oxygen delivery to tissues.\n",
    "\n",
    "Vitamin B12 or Folate Deficiency Anemia: This type of anemia, sometimes called megaloblastic anemia, occurs due to a deficiency in vitamin B12, folate, or both. These vitamins are crucial for DNA synthesis in red blood cells. Causes include poor dietary intake, absorption issues (e.g., pernicious anemia, which is due to the loss of stomach cells producing intrinsic factor needed for B12 absorption), or conditions affecting the small intestine.\n",
    "\n",
    "Hemolytic Anemia: This type of anemia is caused by the premature destruction of red blood cells, which can occur due to autoimmune diseases, genetic conditions (e.g., sickle cell disease or hereditary spherocytosis), infections, or certain medications. In hemolytic anemia, the bone marrow cannot keep up with the rapid loss of RBCs, resulting in low RBC counts and reduced oxygen-carrying capacity.\n",
    "\n",
    "Role of Hemoglobin in Diagnosing Anemia\n",
    "Hemoglobin is a protein in red blood cells that binds to oxygen and transports it throughout the body. In clinical practice, hemoglobin levels are crucial for diagnosing anemia, as low levels indicate insufficient oxygen-carrying capacity. A Complete Blood Count (CBC) test measures hemoglobin levels and provides insights into red blood cell count and size, helping identify the type and severity of anemia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d1a64-fcf1-4d17-ac88-4650ea9a532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    # Review\n",
    "    current_question = event.get('current_question', '')\n",
    "    if current_question:\n",
    "        print(current_question)\n",
    "\n",
    "answer_question = input(\"Answer the question above: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd42c4d8-77b4-43fc-a16f-e586f7d4d957",
   "metadata": {},
   "source": [
    "## Update state with answered question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4f84f-2b69-4b17-8da8-592aa4d525fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.update_state(thread, {\"current_answer\": answer_question}, as_node=\"user_answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a007ab-bd9d-425a-a2fa-c5197a16a8dd",
   "metadata": {},
   "source": [
    "## Run Agent and print the verification or/and teaching results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a01ec2a-2a34-40cd-8766-70b61c55ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    print(graph.get_state(thread).next)\n",
    "    \n",
    "print_verification_results(event)\n",
    "print_teaching_results(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a25340c-1afb-462a-9c53-ba8f849de488",
   "metadata": {},
   "source": [
    "## Answer second question \n",
    "\n",
    "As we can see, we have additional context from the Tavily search, which we did not add at the initial context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b791c-33ec-44a6-85aa-d690cdafcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_store.get_context(event['context_key'])['chunks'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e8e8a-4331-4f3e-933e-79d33846afa1",
   "metadata": {},
   "source": [
    "## Take a look at the second question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd417f-5ace-4e69-aa05-3a10289fb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(event['current_question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee88abd-93e3-44de-9c80-37ccecdf7ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question = input(\"Answer the second question \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df600a63-08da-40eb-a4d8-e703aaf49870",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.update_state(thread, {\"current_answer\": answer_question}, as_node=\"user_answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa5cd6-1682-4863-b8ee-0f80cf4e2f85",
   "metadata": {},
   "source": [
    "## Run agent, see how he treated our lack of knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d6c60-0d57-41a3-93a3-e068c6933b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    print(graph.get_state(thread).next)\n",
    "    \n",
    "print_verification_results(event)\n",
    "print_teaching_results(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20755b0c-7da0-42c0-b3bb-993cb4bcd728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
